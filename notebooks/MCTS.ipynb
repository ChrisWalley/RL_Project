{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipykernel runs in an asyncio loop, so we need to allow nesting\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import asyncio\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from: https://gist.github.com/qpwo/c538c6f73727e254fdc7fab81024f6e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Node:\n",
    "\n",
    "    def __init__(self, state=None, action=None, reward=0, is_terminal=False):\n",
    "\n",
    "        if state is not None:\n",
    "            self.state = state['glyphs_crop']\n",
    "            self.coords = self._get_coordinates(state)\n",
    "            \n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.is_terminal = is_terminal\n",
    "\n",
    "    def _get_coordinates(self, state):\n",
    "        # the first two positions in the blstats\n",
    "        # array give us the row and column coords\n",
    "        col = state['blstats'][0]\n",
    "        row = state['blstats'][1]\n",
    "        return tuple([col, row])\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            exploration_weight=1.0,\n",
    "            num_simulations=50\n",
    "        ):\n",
    "\n",
    "        self.env = env\n",
    "        self.rewards = defaultdict(int)\n",
    "        self.visit_counts = defaultdict(int)\n",
    "        self.children = dict()\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.num_simulations = num_simulations\n",
    "        self.search_depth = 10\n",
    "\n",
    "    def _select(self, node):\n",
    "\n",
    "        # create a list to hold the path\n",
    "        path = []\n",
    "\n",
    "        # loop until we find the path\n",
    "        while True:\n",
    "\n",
    "            # append the current node to the path\n",
    "            path.append(node)\n",
    "\n",
    "            # check for unexplored or terminal nodes\n",
    "            if node not in self.children or not self.children[node]:\n",
    "                return path\n",
    "\n",
    "            # get the unexplored children\n",
    "            unexplored = self.children[node] - self.children.keys()\n",
    "\n",
    "            if unexplored:\n",
    "                n = unexplored.pop()\n",
    "                path.append(n)\n",
    "                return path\n",
    "\n",
    "            # move one layer deeper in the tree\n",
    "            node = self._uct_select(node)\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "\n",
    "        log_N_vertex = math.log(self.visit_counts[node])\n",
    "\n",
    "        def uct(n):\n",
    "            return self.rewards[n] / self.visit_counts[n] + self.exploration_weight * math.sqrt(\n",
    "                log_N_vertex / self.visit_counts[n]\n",
    "            )\n",
    "\n",
    "        return max(self.children[node], key=uct) \n",
    "\n",
    "    def _expand(self, node):\n",
    "\n",
    "        # if the node has already been expanded, \n",
    "        # the we can just return\n",
    "        if node in self.children:\n",
    "            return\n",
    "\n",
    "        # add the node's children to the list\n",
    "        self.children[node] = self._find_children(node)\n",
    "\n",
    "    async def _find_child_async(self, action):\n",
    "\n",
    "        # create a clone of the current environment\n",
    "        env_clone = self.env.clone()\n",
    "\n",
    "        # take the action in the environment\n",
    "        state, reward, is_terminal, _ = env_clone.step(action)\n",
    "\n",
    "        # convert to a tree node\n",
    "        child_node = MCTS_Node(state, action, reward, is_terminal)\n",
    "\n",
    "        # return the child node\n",
    "        return child_node\n",
    "\n",
    "    def _find_children(self, node):\n",
    "\n",
    "        children = set()\n",
    "\n",
    "        for action in range(self.env.action_space.n):\n",
    "\n",
    "            state, reward, is_terminal, _ = self.env.step(action)\n",
    "\n",
    "            child_node = MCTS_Node(state, action, reward, is_terminal)\n",
    "\n",
    "            children.add(child_node)\n",
    "\n",
    "            #time_start = time.perf_counter()\n",
    "\n",
    "            self.env.revert()\n",
    "\n",
    "            #time_stop = time.perf_counter()\n",
    "            #print(f\"{time_stop - time_start:0.4f}\")\n",
    "\n",
    "        return children\n",
    "\n",
    "\n",
    "    # def _find_children(self, node):\n",
    "\n",
    "    #     loop = asyncio.get_event_loop()\n",
    "\n",
    "    #     tasks = []\n",
    "\n",
    "    #     for action in range(self.env.action_space.n):\n",
    "    #         tasks.append(self._find_child_async(action))\n",
    "\n",
    "    #     time_start = time.perf_counter()\n",
    "\n",
    "    #     children = loop.run_until_complete(asyncio.gather(*tasks))\n",
    "\n",
    "    #     time_stop = time.perf_counter()\n",
    "    #     print(f\"{time_stop - time_start:0.4f}\")\n",
    "\n",
    "    #     loop.close()\n",
    "\n",
    "    #     return set(children)\n",
    "\n",
    "\n",
    "    # def _find_children(self, node):\n",
    "\n",
    "    #     children = set()\n",
    "\n",
    "    #     # check if we are done\n",
    "    #     if not node.is_terminal:\n",
    "            \n",
    "    #         # take all actions\n",
    "    #         for action in range(self.env.action_space.n):\n",
    "\n",
    "    #             # create a clone of the current environment\n",
    "    #             #env_clone = self.env.clone()\n",
    "\n",
    "    #             # take the action in the environment\n",
    "    #             #state, reward, is_terminal, _ = env_clone.step(action)\n",
    "\n",
    "    #             # convert to a tree node\n",
    "    #             #child_node = MCTS_Node(state, reward, is_terminal)\n",
    "    #             child_node = MCTS_Node(action = action)\n",
    "\n",
    "    #             # add to the set\n",
    "    #             children.add(child_node)\n",
    "\n",
    "    #     return children\n",
    "\n",
    "    def _simulate(self, node):\n",
    "\n",
    "        # create a clone of the current environment\n",
    "        self.env.revert()\n",
    "\n",
    "        for _ in range(self.search_depth):\n",
    "\n",
    "            if node.is_terminal:\n",
    "                return node.reward\n",
    "\n",
    "            # choose a random action\n",
    "            action = np.random.choice([*range(self.env.action_space.n)])\n",
    "\n",
    "            # take the action in the environment\n",
    "            state, reward, is_terminal, _ = self.env.step(action)\n",
    "\n",
    "            # convert to a tree node\n",
    "            node = MCTS_Node(state, reward, is_terminal)\n",
    "\n",
    "        # if we have not yet reached the end\n",
    "        # return the latest reward\n",
    "        return node.reward\n",
    "\n",
    "    def _backpropogate(self, path, reward):\n",
    "\n",
    "        for node in reversed(path):\n",
    "\n",
    "            self.rewards[node] += reward\n",
    "            self.visit_counts[node] += 1\n",
    "            \n",
    "\n",
    "    def rollout(self, state):\n",
    "\n",
    "        node = MCTS_Node(state=state, action=None)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # run a pre-determined number of simulations\n",
    "        for _ in range(self.num_simulations):\n",
    "\n",
    "            # select a path\n",
    "            path = self._select(node)\n",
    "\n",
    "            # get the leaf node\n",
    "            leaf = path[-1]\n",
    "\n",
    "            # expand the leaf node one level\n",
    "            self._expand(leaf)\n",
    "\n",
    "            # explore to the end for a\n",
    "            # random child of this leaf\n",
    "            reward = self._simulate(leaf)\n",
    "\n",
    "            # back-propogate the reward\n",
    "            self._backpropogate(path, reward)\n",
    "\n",
    "        stop_time = time.perf_counter()\n",
    "        print(f\"{stop_time-start_time:0.4f}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def choose(self, node):\n",
    "        if node.is_terminal:\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "\n",
    "        if node not in self.children:\n",
    "            return node.find_random_child()\n",
    "\n",
    "        def score(n):\n",
    "            if self.visit_counts[n] == 0:\n",
    "                return float(\"-inf\")  # avoid unseen moves\n",
    "            return self.rewards[n] / self.visit_counts[n]  # average reward\n",
    "\n",
    "        return max(self.children[node], key=score)\n",
    "\n",
    "\n",
    "# create the environment\n",
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = -0.001\n",
    ")\n",
    "\n",
    "# create the simulation environment\n",
    "env_s = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = -0.001\n",
    ")\n",
    "\n",
    "# create the search tree\n",
    "tree = MCTS(env_s, exploration_weight = 1.0, num_simulations=1)\n",
    "\n",
    "_ = env.reset()\n",
    "# coords = _get_coordinates(state)\n",
    "# state = state['glyphs_crop']\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    for j in range(50):\n",
    "\n",
    "        state = env_s.revert(env.action_history)\n",
    "\n",
    "    # # do a rollout with update\n",
    "    # node = tree.rollout(state)\n",
    "\n",
    "    # # choose the action\n",
    "    # node = tree.choose(node)\n",
    "\n",
    "    # # take the action\n",
    "    # _ = env.step(node.action)\n",
    "\n",
    "    action = np.random.choice([*range(env.action_space.n)])\n",
    "\n",
    "    _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
