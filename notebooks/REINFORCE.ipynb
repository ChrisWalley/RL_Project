{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "<img src=\"img/algos/REINFORCE.png\" alt=\"REINFORCE\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "from minihack import RewardManager\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "from gym.envs.classic_control import rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "# ---------\n",
    "\n",
    "SEED = 42 # random seed value\n",
    "\n",
    "# actions for the environment\n",
    "MOVE_ACTIONS = tuple(nethack.CompassDirection)\n",
    "NAVIGATE_ACTIONS = MOVE_ACTIONS + (\n",
    "    nethack.Command.SEARCH,\n",
    "    nethack.Command.KICK,\n",
    "    nethack.Command.OPEN,\n",
    "    nethack.Command.LOOK, \n",
    "    nethack.Command.JUMP, \n",
    "    nethack.Command.PICKUP,\n",
    "    nethack.Command.WIELD, \n",
    "    nethack.Command.SWAP,\n",
    "    nethack.Command.EAT,\n",
    "    nethack.Command.ZAP,\n",
    "    nethack.Command.LOOT,\n",
    "    nethack.Command.PUTON,\n",
    "    nethack.Command.APPLY,\n",
    "    nethack.Command.CAST,\n",
    "    nethack.Command.DIP,\n",
    "    nethack.Command.READ,\n",
    "    nethack.Command.INVOKE,\n",
    "    nethack.Command.RUSH,\n",
    "    nethack.Command.WEAR,\n",
    "    nethack.Command.ENHANCE\n",
    "\n",
    ")\n",
    "\n",
    "# maximum number of steps per episode\n",
    "MAX_EPISODE_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for rendering the env as an image\n",
    "class RenderingWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.viewer = rendering.SimpleImageViewer()\n",
    "        self.viewer.width = 1280\n",
    "        self.viewer.height = 520\n",
    "        self.viewer.window = pyglet.window.Window(\n",
    "            width=self.viewer.width, \n",
    "            height=self.viewer.height,\n",
    "            display=self.viewer.display, \n",
    "            vsync=False, \n",
    "            resizable=True\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.pixels = obs['pixel']\n",
    "        return obs['glyphs_crop'], reward, done, info\n",
    "\n",
    "    def render(self, mode=\"human\", **kwargs):\n",
    "        if mode == 'human':\n",
    "            self.viewer.imshow(self.pixels)\n",
    "            return self.viewer.isopen\n",
    "        else:\n",
    "            return self.env.render()\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.pixels = obs['pixel']\n",
    "\n",
    "        # TODO: make sure this is ok\n",
    "        return obs['glyphs_crop']\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.window.close()\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork_v1(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=2, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork_v1, self).__init__()\n",
    "\n",
    "        # Setup the layers of the network\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        # Define simple policy head\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            policy_output_size))\n",
    "\n",
    "        # Define simple value head\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            value_output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # print(\"before: \",x.shape)\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "        \n",
    "        # move forward through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        dist = Categorical(logits=self.policy(out))\n",
    "        value = self.value(out)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork_v2(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=3, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork_v2, self).__init__()\n",
    "\n",
    "        self.shared = nn.Linear(input_size, hidden_size)\n",
    "        self.policy = nn.Linear(hidden_size, policy_output_size)\n",
    "        self.value = nn.Linear(hidden_size, value_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        out = self.shared(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        logits = self.policy(out)\n",
    "        value = self.value(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=3, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "\n",
    "        self.shared = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        self.policy = nn.Linear(hidden_size, policy_output_size)\n",
    "        self.value = nn.Linear(hidden_size, value_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        out = self.shared(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        logits = self.policy(out)\n",
    "        value = self.value(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "        REINFORCE with learned baseline implementation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        - env   :   class   :   online model of the environment\n",
    "        - alpha :   float   :   learning rate\n",
    "        - gamma :   float   :   discount rate\n",
    "        - seed  :   int     :   random seed\n",
    "\n",
    "        References:\n",
    "        -----------\n",
    "\n",
    "        -   Sutton and Barto (2020), \n",
    "            Reinforcement Learning, An Introduction, 2nd Edition, Chapter 13\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env, \n",
    "            policy_model,\n",
    "            alpha,\n",
    "            gamma, \n",
    "            seed,\n",
    "            verbose = True\n",
    "        ):\n",
    "\n",
    "        # set the properties for the agent\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_model = policy_model.to(device)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # set the random seeds\n",
    "        self._seed(seed)\n",
    "\n",
    "        # setup an optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            policy_model.parameters(), \n",
    "            lr = alpha\n",
    "        )\n",
    "\n",
    "    def train(self, num_episodes, max_steps, render=False):\n",
    "\n",
    "        # init a queue to hold the scores\n",
    "        scores = []\n",
    "        queue_scores = deque(maxlen = 100)\n",
    "\n",
    "        # for each episode in the range\n",
    "        for episode in np.arange(start = 1, stop = num_episodes + 1):\n",
    "\n",
    "            # init the lists to hold the values for this episode\n",
    "            policy_loss = []\n",
    "            rewards = []\n",
    "            probabilities = []\n",
    "            values = []\n",
    "\n",
    "            # reset the environment\n",
    "            state = self.env.reset()\n",
    "\n",
    "            # follow a trajectory\n",
    "            for _ in np.arange(start = 1, stop = max_steps + 1 ):\n",
    "\n",
    "                # select the action from the policy for the given state\n",
    "                action, probs, value = self.policy_model.choose_action(state)\n",
    "\n",
    "                # save the probabilities\n",
    "                probabilities.append(probs)\n",
    "                values.append(value)\n",
    "\n",
    "                # take the action in the environment\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # render if required\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                # save the rewards\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # exit if we have reached the end\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # calculate the total rewards we achieved\n",
    "            total_rewards = sum(rewards)\n",
    "            \n",
    "            # save them for later\n",
    "            scores.append(total_rewards)\n",
    "            queue_scores.append(total_rewards)\n",
    "\n",
    "            # compute the returns (baseline)\n",
    "            G = self._compute_returns(rewards, self.gamma)\n",
    "            G = torch.from_numpy(G).float().to(device)\n",
    "\n",
    "            # calculate the loss for the policy\n",
    "            total_value = torch.cat(values)\n",
    "            probabilities = torch.cat(probabilities)\n",
    "            delta = G - total_value\n",
    "\n",
    "            # loss functions\n",
    "            policy_loss = -torch.sum(probabilities*delta.detach())\n",
    "            value_loss = 0.5 * torch.sum(delta**2)\n",
    "            loss = policy_loss + value_loss # total\n",
    "\n",
    "            # propogate backwards and proceed to the next step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.verbose:# and episode % 50 == 0:\n",
    "                print('Episode {}\\tAverage Score: {:.2f}'.format(\n",
    "                    episode, np.mean(queue_scores)\n",
    "                ))\n",
    "\n",
    "        # return the data\n",
    "        return self.policy_model, scores\n",
    "\n",
    "    def _compute_returns(self, rewards, gamma):\n",
    "\n",
    "        # init\n",
    "        r = 0\n",
    "        returns = []\n",
    "\n",
    "        # discount over the trajectory\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            r = rewards[step] + gamma * r\n",
    "            returns.insert(0, r)\n",
    "\n",
    "        # ensure type \n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # calculate the mean and std\n",
    "        mean = returns.mean(axis = 0)\n",
    "        std = returns.std(axis = 0)\n",
    "\n",
    "        # average and normalize the returns\n",
    "        if std == 0.0:\n",
    "            returns = 0.0\n",
    "        else:\n",
    "            returns = (returns - mean) / std\n",
    "\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def _seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.env.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.03\n",
      "Episode 2\tAverage Score: 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m policy_model \u001b[39m=\u001b[39m PolicyValueNetwork()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m agent \u001b[39m=\u001b[39m REINFORCE(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     env, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     policy_model\u001b[39m=\u001b[39mpolicy_model, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     max_steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 11\u001b[0m in \u001b[0;36mREINFORCE.train\u001b[0;34m(self, num_episodes, max_steps, render)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m values\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# take the action in the environment\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m state, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# render if required\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m render:\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 11\u001b[0m in \u001b[0;36mRenderingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpixels \u001b[39m=\u001b[39m obs[\u001b[39m'\u001b[39m\u001b[39mpixel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obs[\u001b[39m'\u001b[39m\u001b[39mglyphs_crop\u001b[39m\u001b[39m'\u001b[39m], reward, done, info\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/minihack/base.py:343\u001b[0m, in \u001b[0;36mMiniHack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_previous_action \u001b[39m=\u001b[39m action\n\u001b[1;32m    341\u001b[0m \u001b[39m# Within this call, _is_episode_end is called and then _reward_fn,\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m# both using self.reward_manager\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/nle/env/base.py:420\u001b[0m, in \u001b[0;36mNLE.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    417\u001b[0m info[\u001b[39m\"\u001b[39m\u001b[39mend_status\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m end_status\n\u001b[1;32m    418\u001b[0m info[\u001b[39m\"\u001b[39m\u001b[39mis_ascended\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mhow_done() \u001b[39m==\u001b[39m nethack\u001b[39m.\u001b[39mASCENDED\n\u001b[0;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_observation(observation), reward, done, info\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/minihack/base.py:418\u001b[0m, in \u001b[0;36mMiniHack._get_observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    413\u001b[0m         obs_dict[key] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_crop_observation(\n\u001b[1;32m    414\u001b[0m             observation[orig_key], loc\n\u001b[1;32m    415\u001b[0m         )\n\u001b[1;32m    417\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpixel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_minihack_obs_keys:\n\u001b[0;32m--> 418\u001b[0m     obs_dict[\u001b[39m\"\u001b[39m\u001b[39mpixel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_glyph_mapper\u001b[39m.\u001b[39;49mto_rgb(\n\u001b[1;32m    419\u001b[0m         observation[\u001b[39m\"\u001b[39;49m\u001b[39mglyphs\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpixel_crop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_minihack_obs_keys:\n\u001b[1;32m    423\u001b[0m     obs_dict[\u001b[39m\"\u001b[39m\u001b[39mpixel_crop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_glyph_mapper\u001b[39m.\u001b[39mto_rgb(\n\u001b[1;32m    424\u001b[0m         obs_dict[\u001b[39m\"\u001b[39m\u001b[39mglyphs_crop\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    425\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/minihack/tiles/glyph_mapper.py:56\u001b[0m, in \u001b[0;36mGlyphMapper.to_rgb\u001b[0;34m(self, glyphs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_rgb\u001b[39m(\u001b[39mself\u001b[39m, glyphs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_glyph_to_rgb(glyphs)\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/minihack/tiles/glyph_mapper.py:45\u001b[0m, in \u001b[0;36mGlyphMapper._glyph_to_rgb\u001b[0;34m(self, glyphs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         col \u001b[39m=\u001b[39m rgb\n\u001b[1;32m     44\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         col \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((col, rgb))\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m cols \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     cols \u001b[39m=\u001b[39m col\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the environment\n",
    "# https://minihack.readthedocs.io/en/latest/envs/skills/quest.html\n",
    "\n",
    "# setup the reward manager\n",
    "# https://minihack.readthedocs.io/en/latest/getting-started/reward.html?highlight=RewardManager#reward-manager\n",
    "reward_manager = RewardManager()\n",
    "reward_manager.add_kill_event(\"minotaur\", reward=10)\n",
    "reward_manager.add_kill_event(\"goblin\", reward=1)\n",
    "reward_manager.add_kill_event(\"jackal\", reward=1)\n",
    "reward_manager.add_kill_event(\"giant rat\", reward=1)\n",
    "\n",
    "# make the environment\n",
    "env = gym.make(\n",
    "    \"MiniHack-Quest-Hard-v0\",\n",
    "    actions = NAVIGATE_ACTIONS,\n",
    "    reward_manager = reward_manager,\n",
    "    observation_keys = (\"glyphs\", \"pixel\", \"glyphs_crop\"),\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = 0.002,\n",
    ")\n",
    "env.seed(SEED)\n",
    "\n",
    "# wrappers\n",
    "env = RenderingWrapper(env)\n",
    "\n",
    "policy_model = PolicyValueNetwork()\n",
    "\n",
    "agent = REINFORCE(\n",
    "    env, \n",
    "    policy_model=policy_model, \n",
    "    alpha=0.02,\n",
    "    gamma=0.99,\n",
    "    seed=SEED, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    num_episodes=1000,\n",
    "    max_steps=1000,\n",
    "    render=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('.cenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
