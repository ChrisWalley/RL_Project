{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "<img src=\"img/algos/REINFORCE.png\" alt=\"REINFORCE\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "from minihack import RewardManager\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "from gym.envs.classic_control import rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "# ---------\n",
    "\n",
    "SEED = 0 # random seed value\n",
    "\n",
    "# actions for the environment\n",
    "MOVE_ACTIONS = tuple(nethack.CompassDirection)\n",
    "NAVIGATE_ACTIONS = MOVE_ACTIONS + (\n",
    "    nethack.Command.OPEN,\n",
    "    nethack.Command.KICK,\n",
    "    nethack.Command.SEARCH,\n",
    ")\n",
    "\n",
    "# maximum number of steps per episode\n",
    "MAX_EPISODE_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for rendering the env as an image\n",
    "class RenderingWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.viewer = rendering.SimpleImageViewer()\n",
    "        self.viewer.width = 1280\n",
    "        self.viewer.height = 520\n",
    "        self.viewer.window = pyglet.window.Window(\n",
    "            width=self.viewer.width, \n",
    "            height=self.viewer.height,\n",
    "            display=self.viewer.display, \n",
    "            vsync=False, \n",
    "            resizable=True\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.pixels = obs['pixel']\n",
    "        return obs['glyphs_crop'], reward, done, info\n",
    "\n",
    "    def render(self, mode=\"human\", **kwargs):\n",
    "        if mode == 'human':\n",
    "            self.viewer.imshow(self.pixels)\n",
    "            return self.viewer.isopen\n",
    "        else:\n",
    "            return self.env.render()\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.pixels = obs['pixel']\n",
    "\n",
    "        # TODO: make sure this is ok\n",
    "        return obs['glyphs_crop']\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.window.close()\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=1, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "\n",
    "        # Setup the layers of the network\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        # Define simple policy head\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            policy_output_size))\n",
    "\n",
    "        # Define simple value head\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            value_output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # print(\"before: \",x.shape)\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "        \n",
    "        # move forward through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        dist = Categorical(logits=self.policy(out))\n",
    "        value = self.value(out)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "        REINFORCE with learned baseline implementation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        - env   :   class   :   online model of the environment\n",
    "        - alpha :   float   :   learning rate\n",
    "        - gamma :   float   :   discount rate\n",
    "        - seed  :   int     :   random seed\n",
    "\n",
    "        References:\n",
    "        -----------\n",
    "\n",
    "        -   Sutton and Barto (2020), \n",
    "            Reinforcement Learning, An Introduction, 2nd Edition, Chapter 13\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env, \n",
    "            policy_model,\n",
    "            alpha,\n",
    "            gamma, \n",
    "            seed,\n",
    "            verbose = True\n",
    "        ):\n",
    "\n",
    "        # set the properties for the agent\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_model = policy_model.to(device)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # set the random seeds\n",
    "        self._seed(seed)\n",
    "\n",
    "        # setup an optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            policy_model.parameters(), \n",
    "            lr = alpha\n",
    "        )\n",
    "\n",
    "    def train(self, num_episodes, max_steps):\n",
    "\n",
    "        # init a queue to hold the scores\n",
    "        scores = []\n",
    "        queue_scores = deque(maxlen = 100)\n",
    "\n",
    "        # for each episode in the range\n",
    "        for episode in np.arange(start = 1, stop = num_episodes + 1):\n",
    "\n",
    "            # init the lists to hold the values for this episode\n",
    "            policy_loss = []\n",
    "            rewards = []\n",
    "            probabilities = []\n",
    "            values = []\n",
    "\n",
    "            # reset the environment\n",
    "            state = self.env.reset()\n",
    "\n",
    "            # follow a trajectory\n",
    "            for _ in np.arange(start = 1, stop = max_steps + 1 ):\n",
    "\n",
    "                # select the action from the policy for the given state\n",
    "                action, probs, value = self.policy_model.choose_action(state)\n",
    "\n",
    "                # save the probabilities\n",
    "                probabilities.append(probs)\n",
    "                values.append(value)\n",
    "\n",
    "                # take the action in the environment\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # save the rewards\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # exit if we have reached the end\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # calculate the total rewards we achieved\n",
    "            total_rewards = sum(rewards)\n",
    "            \n",
    "            # save them for later\n",
    "            scores.append(total_rewards)\n",
    "            queue_scores.append(total_rewards)\n",
    "\n",
    "            # compute the returns (baseline)\n",
    "            G = self._compute_returns(rewards, self.gamma)\n",
    "            G = torch.from_numpy(G).float().to(device)\n",
    "\n",
    "            # calculate the loss for the policy\n",
    "            total_value = torch.cat(values)\n",
    "            probabilities = torch.cat(probabilities)\n",
    "            delta = G - total_value\n",
    "\n",
    "            # loss functions\n",
    "            policy_loss = -torch.sum(probabilities*delta.detach())\n",
    "            value_loss = 0.5 * torch.sum(delta**2)\n",
    "            loss = policy_loss + value_loss # total\n",
    "\n",
    "            # propogate backwards and proceed to the next step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.verbose:# and episode % 50 == 0:\n",
    "                print('Episode {}\\tAverage Score: {:.2f}'.format(\n",
    "                    episode, np.mean(queue_scores)\n",
    "                ))\n",
    "\n",
    "        # return the data\n",
    "        return self.policy_model, scores\n",
    "\n",
    "    def _compute_returns(self, rewards, gamma):\n",
    "\n",
    "        # init\n",
    "        r = 0\n",
    "        returns = []\n",
    "\n",
    "        # discount over the trajectory\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            r = rewards[step] + gamma * r\n",
    "            returns.insert(0, r)\n",
    "\n",
    "        # ensure type \n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # calculate the mean and std\n",
    "        mean = returns.mean(axis = 0)\n",
    "        std = returns.std(axis = 0)\n",
    "\n",
    "        # average and normalize the returns\n",
    "        returns = (returns - mean) / std\n",
    "\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def _seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.env.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "# https://minihack.readthedocs.io/en/latest/envs/skills/quest.html\n",
    "\n",
    "# setup the reward manager\n",
    "# https://minihack.readthedocs.io/en/latest/getting-started/reward.html?highlight=RewardManager#reward-manager\n",
    "reward_manager = RewardManager()\n",
    "reward_manager.add_kill_event(\"minotaur\", reward=10)\n",
    "reward_manager.add_kill_event(\"goblin\", reward=1)\n",
    "reward_manager.add_kill_event(\"jackal\", reward=1)\n",
    "reward_manager.add_kill_event(\"giant rat\", reward=1)\n",
    "\n",
    "# make the environment\n",
    "env = gym.make(\n",
    "    \"MiniHack-Quest-Hard-v0\",\n",
    "    actions=NAVIGATE_ACTIONS,\n",
    "    reward_manager=reward_manager,\n",
    "    observation_keys=(\"glyphs\", \"pixel\", \"glyphs_crop\"),\n",
    ")\n",
    "env.seed(SEED)\n",
    "\n",
    "# wrappers\n",
    "env = RenderingWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51\tAverage Score: -0.50\n"
     ]
    }
   ],
   "source": [
    "policy_model = PolicyValueNetwork()\n",
    "\n",
    "agent = REINFORCE(\n",
    "    env, \n",
    "    policy_model=policy_model, \n",
    "    alpha=0.02, \n",
    "    gamma=0.99, \n",
    "    seed=SEED, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    num_episodes=1000,\n",
    "    max_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('.cenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
