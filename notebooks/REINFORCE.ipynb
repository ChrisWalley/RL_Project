{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "<img src=\"img/algos/REINFORCE.png\" alt=\"REINFORCE\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork_v1(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=2, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork_v1, self).__init__()\n",
    "\n",
    "        # Setup the layers of the network\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        # Define simple policy head\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            policy_output_size))\n",
    "\n",
    "        # Define simple value head\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            value_output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # print(\"before: \",x.shape)\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "        \n",
    "        # move forward through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        dist = Categorical(logits=self.policy(out))\n",
    "        value = self.value(out)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork_v2(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=3, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork_v2, self).__init__()\n",
    "\n",
    "        self.shared = nn.Linear(input_size, hidden_size)\n",
    "        self.policy = nn.Linear(hidden_size, policy_output_size)\n",
    "        self.value = nn.Linear(hidden_size, value_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        out = self.shared(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        logits = self.policy(out)\n",
    "        value = self.value(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        #logits = F.softmax(logits, dim=1)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "        print(f\"Action: {action}\")\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "        REINFORCE with learned baseline implementation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        - env   :   class   :   online model of the environment\n",
    "        - alpha :   float   :   learning rate\n",
    "        - gamma :   float   :   discount rate\n",
    "        - seed  :   int     :   random seed\n",
    "\n",
    "        References:\n",
    "        -----------\n",
    "\n",
    "        -   Sutton and Barto (2020), \n",
    "            Reinforcement Learning, An Introduction, 2nd Edition, Chapter 13\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env, \n",
    "            policy_model,\n",
    "            alpha,\n",
    "            gamma, \n",
    "            seed,\n",
    "            verbose = True\n",
    "        ):\n",
    "\n",
    "        # set the properties for the agent\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_model = policy_model.to(device)\n",
    "        self.verbose = verbose\n",
    "        self.visit_counts = dict()\n",
    "        self.reward_totals = dict()\n",
    "\n",
    "        # set the random seeds\n",
    "        self._seed(self.env.seed_value)\n",
    "\n",
    "        # setup an optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            policy_model.parameters(), \n",
    "            lr = alpha\n",
    "        )\n",
    "\n",
    "    def _get_coordinates(self, state):\n",
    "\n",
    "        col = state['blstats'][0]\n",
    "        row = state['blstats'][1]\n",
    "\n",
    "        return tuple([col, row])\n",
    "\n",
    "    def _get_uct_reward(self, state, reward):\n",
    "        coords = self._get_coordinates(state)\n",
    "\n",
    "        if coords not in self.visit_counts:\n",
    "            self.visit_counts[coords] = 1\n",
    "            self.reward_totals[coords] = reward\n",
    "        else:\n",
    "            self.visit_counts[coords] += 1\n",
    "            self.reward_totals[coords] += reward\n",
    "\n",
    "        r = reward\n",
    "        r += self.reward_totals[coords] / self.visit_counts[coords]\n",
    "        r += 1. * math.sqrt( \\\n",
    "            (math.log(self.visit_counts[coords])) \\\n",
    "            / \\\n",
    "            self.visit_counts[coords])\n",
    "\n",
    "        return r\n",
    "\n",
    "\n",
    "    def train(self, num_episodes, max_steps, render=False):\n",
    "\n",
    "        # init a queue to hold the scores\n",
    "        scores = []\n",
    "        queue_scores = deque(maxlen = 100)\n",
    "\n",
    "        # for each episode in the range\n",
    "        for episode in np.arange(start = 1, stop = num_episodes + 1):\n",
    "\n",
    "            # init the lists to hold the values for this episode\n",
    "            policy_loss = []\n",
    "            rewards = []\n",
    "            probabilities = []\n",
    "            values = []\n",
    "\n",
    "            # reset the environment\n",
    "            state = self.env.reset()\n",
    "            state = state['glyphs_crop']\n",
    "\n",
    "            steps = 0\n",
    "\n",
    "            # follow a trajectory\n",
    "            for step in np.arange(start = 1, stop = max_steps + 1 ):\n",
    "\n",
    "                # select the action from the policy for the given state\n",
    "                action, probs, value = self.policy_model.choose_action(state)\n",
    "\n",
    "                # save the probabilities\n",
    "                probabilities.append(probs)\n",
    "                values.append(value)\n",
    "\n",
    "                # take the action in the environment\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                #reward = self._get_uct_reward(state, reward)\n",
    "                state = state['glyphs_crop']\n",
    "\n",
    "                # render if required\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                # save the rewards\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # exit if we have reached the end\n",
    "                if done:\n",
    "                    steps = step\n",
    "                    break\n",
    "\n",
    "            # calculate the total rewards we achieved\n",
    "            total_rewards = sum(rewards)\n",
    "            \n",
    "            # save them for later\n",
    "            scores.append(total_rewards)\n",
    "            queue_scores.append(total_rewards)\n",
    "\n",
    "            # compute the returns (baseline)\n",
    "            G = self._compute_returns(rewards, self.gamma)\n",
    "            G = torch.from_numpy(G).float().to(device)\n",
    "\n",
    "            # calculate the loss for the policy\n",
    "            total_value = torch.cat(values)\n",
    "            probabilities = torch.cat(probabilities)\n",
    "            delta = G - total_value\n",
    "\n",
    "            # loss functions\n",
    "            policy_loss = -torch.sum( probabilities * delta.detach() )\n",
    "            value_loss = 0.5 * torch.sum(delta**2)\n",
    "            loss = policy_loss + value_loss # total\n",
    "\n",
    "            # propogate backwards and proceed to the next step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.verbose:# and episode % 50 == 0:\n",
    "                print( f\"Episode: {episode}, Steps: {steps}, Average Score: {np.mean(queue_scores)}\")\n",
    "\n",
    "        # return the data\n",
    "        return self.policy_model, scores\n",
    "\n",
    "    def _compute_returns(self, rewards, gamma):\n",
    "\n",
    "        # init\n",
    "        r = 0\n",
    "        returns = []\n",
    "\n",
    "        # discount over the trajectory\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            r = rewards[step] + gamma * r\n",
    "            returns.insert(0, r)\n",
    "\n",
    "        # ensure type \n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # calculate the mean and std\n",
    "        mean = returns.mean(axis = 0)\n",
    "        std = returns.std(axis = 0)\n",
    "\n",
    "        # average and normalize the returns\n",
    "        if std == 0.0:\n",
    "            returns = 0.0\n",
    "        else:\n",
    "            returns = (returns - mean) / std\n",
    "\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def _seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=162,\n",
    "        num_hidden_layers=3, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "\n",
    "        self.shared = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        self.policy = nn.Linear(hidden_size, policy_output_size)\n",
    "        self.value = nn.Linear(hidden_size, value_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        out = self.shared(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        logits = self.policy(out)\n",
    "        value = self.value(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value\n",
    "\n",
    "\n",
    "MAX_STEPS = 5000\n",
    "\n",
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = 0.005,\n",
    "    max_episode_steps = MAX_STEPS\n",
    ")\n",
    "\n",
    "policy_model = PolicyValueNetwork(\n",
    "    input_size=441,\n",
    "    policy_output_size=env.action_space.n)\n",
    "\n",
    "agent = REINFORCE(\n",
    "    env, \n",
    "    policy_model=policy_model, \n",
    "    alpha = 0.1,\n",
    "    gamma = 0.90,\n",
    "    seed = 0, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    num_episodes=10000,\n",
    "    max_steps=MAX_STEPS,\n",
    "    render=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('.cenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
