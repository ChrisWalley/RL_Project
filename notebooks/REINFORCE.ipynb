{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "<img src=\"img/algos/REINFORCE.png\" alt=\"REINFORCE\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork_v1(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=2, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork_v1, self).__init__()\n",
    "\n",
    "        # Setup the layers of the network\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        # Define simple policy head\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            policy_output_size))\n",
    "\n",
    "        # Define simple value head\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size,\n",
    "            value_output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # print(\"before: \",x.shape)\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "        \n",
    "        # move forward through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        dist = Categorical(logits=self.policy(out))\n",
    "        value = self.value(out)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork_v2(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=3, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork_v2, self).__init__()\n",
    "\n",
    "        self.shared = nn.Linear(input_size, hidden_size)\n",
    "        self.policy = nn.Linear(hidden_size, policy_output_size)\n",
    "        self.value = nn.Linear(hidden_size, value_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        out = self.shared(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        logits = self.policy(out)\n",
    "        value = self.value(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        #logits = F.softmax(logits, dim=1)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "        print(f\"Action: {action}\")\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "        REINFORCE with learned baseline implementation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        - env   :   class   :   online model of the environment\n",
    "        - alpha :   float   :   learning rate\n",
    "        - gamma :   float   :   discount rate\n",
    "        - seed  :   int     :   random seed\n",
    "\n",
    "        References:\n",
    "        -----------\n",
    "\n",
    "        -   Sutton and Barto (2020), \n",
    "            Reinforcement Learning, An Introduction, 2nd Edition, Chapter 13\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env, \n",
    "            policy_model,\n",
    "            alpha,\n",
    "            gamma, \n",
    "            seed,\n",
    "            verbose = True\n",
    "        ):\n",
    "\n",
    "        # set the properties for the agent\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.policy_model = policy_model.to(device)\n",
    "        self.verbose = verbose\n",
    "        self.visit_counts = dict()\n",
    "        self.reward_totals = dict()\n",
    "\n",
    "        # set the random seeds\n",
    "        self._seed(self.env.seed_value)\n",
    "\n",
    "        # setup an optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            policy_model.parameters(), \n",
    "            lr = alpha\n",
    "        )\n",
    "\n",
    "    def _get_coordinates(self, state):\n",
    "\n",
    "        col = state['blstats'][0]\n",
    "        row = state['blstats'][1]\n",
    "\n",
    "        return tuple([col, row])\n",
    "\n",
    "    def _get_uct_reward(self, state, reward):\n",
    "        coords = self._get_coordinates(state)\n",
    "\n",
    "        if coords not in self.visit_counts:\n",
    "            self.visit_counts[coords] = 1\n",
    "            self.reward_totals[coords] = reward\n",
    "        else:\n",
    "            self.visit_counts[coords] += 1\n",
    "            self.reward_totals[coords] += reward\n",
    "\n",
    "        r = reward\n",
    "        r += self.reward_totals[coords] / self.visit_counts[coords]\n",
    "        r += 1. * math.sqrt( \\\n",
    "            (math.log(self.visit_counts[coords])) \\\n",
    "            / \\\n",
    "            self.visit_counts[coords])\n",
    "\n",
    "        return r\n",
    "\n",
    "\n",
    "    def train(self, num_episodes, max_steps, render=False):\n",
    "\n",
    "        # init a queue to hold the scores\n",
    "        scores = []\n",
    "        queue_scores = deque(maxlen = 100)\n",
    "\n",
    "        # for each episode in the range\n",
    "        for episode in np.arange(start = 1, stop = num_episodes + 1):\n",
    "\n",
    "            # init the lists to hold the values for this episode\n",
    "            policy_loss = []\n",
    "            rewards = []\n",
    "            probabilities = []\n",
    "            values = []\n",
    "\n",
    "            # reset the environment\n",
    "            state = self.env.reset()\n",
    "            state = state['glyphs_crop']\n",
    "\n",
    "            # follow a trajectory\n",
    "            for step in np.arange(start = 1, stop = max_steps + 1 ):\n",
    "\n",
    "                # select the action from the policy for the given state\n",
    "                action, probs, value = self.policy_model.choose_action(state)\n",
    "\n",
    "                # save the probabilities\n",
    "                probabilities.append(probs)\n",
    "                values.append(value)\n",
    "\n",
    "                # take the action in the environment\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                #reward = self._get_uct_reward(state, reward)\n",
    "                state = state['glyphs_crop']\n",
    "\n",
    "                # render if required\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                # save the rewards\n",
    "                rewards.append(reward)\n",
    "\n",
    "                # exit if we have reached the end\n",
    "                if done:\n",
    "                    print(f\"Steps: {step}\")\n",
    "                    break\n",
    "\n",
    "            # calculate the total rewards we achieved\n",
    "            total_rewards = sum(rewards)\n",
    "            \n",
    "            # save them for later\n",
    "            scores.append(total_rewards)\n",
    "            queue_scores.append(total_rewards)\n",
    "\n",
    "            # compute the returns (baseline)\n",
    "            G = self._compute_returns(rewards, self.gamma)\n",
    "            G = torch.from_numpy(G).float().to(device)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            # calculate the loss for the policy\n",
    "            total_value = torch.cat(values)\n",
    "            probabilities = torch.cat(probabilities)\n",
    "            delta = G - total_value\n",
    "\n",
    "            # loss functions\n",
    "            policy_loss = -torch.sum(probabilities*delta.detach())\n",
    "            value_loss = 0.5 * torch.sum(delta**2)\n",
    "            loss = policy_loss + value_loss # total\n",
    "\n",
    "            # propogate backwards and proceed to the next step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.verbose:# and episode % 50 == 0:\n",
    "                print('Episode {}\\tAverage Score: {:.2f}'.format(\n",
    "                    episode, np.mean(queue_scores)\n",
    "                ))\n",
    "\n",
    "        # return the data\n",
    "        return self.policy_model, scores\n",
    "\n",
    "    def _compute_returns(self, rewards, gamma):\n",
    "\n",
    "        # init\n",
    "        r = 0\n",
    "        returns = []\n",
    "\n",
    "        # discount over the trajectory\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            r = rewards[step] + gamma * r\n",
    "            returns.insert(0, r)\n",
    "\n",
    "        # ensure type \n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # calculate the mean and std\n",
    "        mean = returns.mean(axis = 0)\n",
    "        std = returns.std(axis = 0)\n",
    "\n",
    "        # average and normalize the returns\n",
    "        if std == 0.0:\n",
    "            returns = 0.0\n",
    "        else:\n",
    "            returns = (returns - mean) / std\n",
    "\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def _seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x441 and 81x162)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m policy_model \u001b[39m=\u001b[39m PolicyValueNetwork(policy_output_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m agent \u001b[39m=\u001b[39m REINFORCE(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     env, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     policy_model\u001b[39m=\u001b[39mpolicy_model, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     max_steps\u001b[39m=\u001b[39;49mMAX_STEPS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 8\u001b[0m in \u001b[0;36mREINFORCE.train\u001b[0;34m(self, num_episodes, max_steps, render)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# follow a trajectory\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(start \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, stop \u001b[39m=\u001b[39m max_steps \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m ):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39m# select the action from the policy for the given state\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     action, probs, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_model\u001b[39m.\u001b[39;49mchoose_action(state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39m# save the probabilities\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     probabilities\u001b[39m.\u001b[39mappend(probs)\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 8\u001b[0m in \u001b[0;36mPolicyValueNetwork.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# move forward through the network\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# and get the distribution and value\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m dist, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(Variable(state))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# choose a random sample action\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m action \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39msample()\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb Cell 8\u001b[0m in \u001b[0;36mPolicyValueNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(x, (\u001b[39m1\u001b[39m,x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(x, p\u001b[39m=\u001b[39m\u001b[39m2.0\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, eps\u001b[39m=\u001b[39m\u001b[39m1e-12\u001b[39m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy(out)\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x441 and 81x162)"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size=81, \n",
    "        hidden_size=162,\n",
    "        num_hidden_layers=3, \n",
    "        policy_output_size=4, \n",
    "        value_output_size=1\n",
    "    ):\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "\n",
    "        self.shared = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in np.arange(num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        self.policy = nn.Linear(hidden_size, policy_output_size)\n",
    "        self.value = nn.Linear(hidden_size, value_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = torch.reshape(x, (1,x.shape[0]))\n",
    "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        out = self.shared(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        logits = self.policy(out)\n",
    "        value = self.value(out)\n",
    "\n",
    "        # get the output distribution\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # return the distribution and value\n",
    "        return dist, value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # get the state\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().to(device).unsqueeze(0)\n",
    "\n",
    "        # move forward through the network\n",
    "        # and get the distribution and value\n",
    "        dist, value = self.forward(Variable(state))\n",
    "\n",
    "        # choose a random sample action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # return the action and the probabilities for the action and the value\n",
    "        return action.item(), dist.log_prob(action), value\n",
    "\n",
    "\n",
    "MAX_STEPS = 5000\n",
    "\n",
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = -0.001,\n",
    "    max_episode_steps = MAX_STEPS\n",
    ")\n",
    "\n",
    "policy_model = PolicyValueNetwork(\n",
    "    input_size=441,\n",
    "    policy_output_size=env.action_space.n)\n",
    "\n",
    "agent = REINFORCE(\n",
    "    env, \n",
    "    policy_model=policy_model, \n",
    "    alpha = 0.1,\n",
    "    gamma = 0.90,\n",
    "    seed = 0, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    num_episodes=1000,\n",
    "    max_steps=MAX_STEPS,\n",
    "    render=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('.cenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
