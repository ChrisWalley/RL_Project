{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE v.2\n",
    "\n",
    "![](img/algos/REINFORCE_v2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nle import nethack\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1.  Avg Rewards: -9.998000000000008\n",
      "Episode 2.  Avg Rewards: -9.998000000000008\n",
      "Episode 3.  Avg Rewards: -9.998000000000008\n",
      "Episode 4.  Avg Rewards: -9.998000000000008\n",
      "Episode 5.  Avg Rewards: -9.998000000000008\n",
      "Episode 6.  Avg Rewards: -9.998000000000006\n",
      "Episode 7.  Avg Rewards: -9.998000000000006\n",
      "Episode 8.  Avg Rewards: -9.998000000000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/n99nr9kx3fd5d975tlfwmh6r0000gn/T/ipykernel_15666/3290982794.py:110: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  state_tensor = torch.FloatTensor(batch_states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9.  Avg Rewards: -9.998000000000008\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 163>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m     input_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mspaces[OBS_SPACE]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m                 env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mspaces[OBS_SPACE]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m policy_model \u001b[39m=\u001b[39m PolicyNetwork(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     env \u001b[39m=\u001b[39m env,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     input_size \u001b[39m=\u001b[39m input_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     hidden_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mspaces[OBS_SPACE]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     output_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m REINFORCE(env, policy_model, num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, max_steps\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb Cell 4\u001b[0m in \u001b[0;36mREINFORCE\u001b[0;34m(env, policy_model, num_episodes, max_steps, batch_size, alpha, gamma, render)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m state \u001b[39m=\u001b[39m state[OBS_SPACE]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_steps):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     action \u001b[39m=\u001b[39m policy_model\u001b[39m.\u001b[39;49mchoose_action(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     next_state \u001b[39m=\u001b[39m next_state[OBS_SPACE]\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb Cell 4\u001b[0m in \u001b[0;36mPolicyNetwork.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m probs \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m action_space \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(action_space, p \u001b[39m=\u001b[39;49m probs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v2.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[0;32mmtrand.pyx:935\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class PolicyNetwork:\n",
    "\n",
    "    def __init__(self, env, input_size, hidden_size, output_size):\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "    def prepare_state(self, state):\n",
    "\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.float().unsqueeze(0)\n",
    "\n",
    "        state = torch.flatten(state)\n",
    "        state = torch.reshape(state, (1, state.shape[0]))\n",
    "        state = state.detach().numpy()[0]\n",
    "        #state = torch.nn.functional.normalize(state, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        state = self.prepare_state(state)\n",
    "        return self.net(torch.FloatTensor(state))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        probs = self.forward(state)\n",
    "        probs = probs.detach().numpy()\n",
    "        action_space = np.arange(self.env.action_space.n)\n",
    "        action = np.random.choice(action_space, p = probs)\n",
    "        return action\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "        for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    return r - r.mean()\n",
    "\n",
    "#OBS_SPACE = 'blstats'\n",
    "OBS_SPACE = 'glyphs_crop'\n",
    "\n",
    "def REINFORCE(env, policy_model, num_episodes, max_steps, batch_size, alpha, gamma, render=False):\n",
    "\n",
    "    optimizer = optim.Adam(policy_model.net.parameters(), lr = alpha)\n",
    "\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "\n",
    "    for k in range(1, num_episodes + 1):\n",
    "\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "\n",
    "        state = env.reset()\n",
    "        state = state[OBS_SPACE]\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "\n",
    "            action = policy_model.choose_action(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = next_state[OBS_SPACE]\n",
    "\n",
    "            # render if required\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            states.append(policy_model.prepare_state(state))\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # batch\n",
    "        batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "        batch_states.extend(states)\n",
    "        batch_actions.extend(actions)\n",
    "        batch_counter += 1\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # If batch is complete, update network\n",
    "        if batch_counter == batch_size:\n",
    "            optimizer.zero_grad()\n",
    "            state_tensor = torch.FloatTensor(batch_states)\n",
    "            reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "            # Actions are used as indices, must be \n",
    "            # LongTensor\n",
    "            action_tensor = torch.LongTensor(batch_actions)\n",
    "\n",
    "            # Calculate loss\n",
    "            pred = policy_model.net(state_tensor)\n",
    "            logprob = torch.log(pred)\n",
    "            selected_logprobs = reward_tensor * \\\n",
    "                torch.gather(logprob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
    "            loss = -selected_logprobs.mean()\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_rewards = []\n",
    "            batch_actions = []\n",
    "            batch_states = []\n",
    "            batch_counter = 1\n",
    "\n",
    "        if k % 1 == 0:\n",
    "            avg_rewards = np.mean(total_rewards[-100:])\n",
    "            print( f\"Episode {k}.  Avg Rewards: {avg_rewards}\" )\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "MAX_STEPS = 5000\n",
    "\n",
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = -0.001,\n",
    "    max_episode_steps = MAX_STEPS\n",
    ")\n",
    "\n",
    "if len(env.observation_space.spaces[OBS_SPACE].shape) == 1:\n",
    "    input_size = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "else:\n",
    "    input_size = env.observation_space.spaces[OBS_SPACE].shape[0] * \\\n",
    "                env.observation_space.spaces[OBS_SPACE].shape[1]\n",
    "\n",
    "policy_model = PolicyNetwork(\n",
    "    env = env,\n",
    "    input_size = input_size,\n",
    "    hidden_size = env.observation_space.spaces[OBS_SPACE].shape[0] * 2,\n",
    "    output_size = env.action_space.n\n",
    ")\n",
    "\n",
    "REINFORCE(env, policy_model, num_episodes=1000, max_steps=5000, batch_size=10, alpha=0.01, gamma=0.99, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
