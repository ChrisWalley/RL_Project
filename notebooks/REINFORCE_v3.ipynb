{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discount factor for future utilities\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "#number of episodes to run\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "#max steps per episode\n",
    "MAX_STEPS = 5000\n",
    "\n",
    "#score agent needs for environment to be solved\n",
    "SOLVED_SCORE = 195\n",
    "\n",
    "#device to run model on \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn our policy parameters\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in observations and outputs actions\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(observation_space, observation_space * 2)\n",
    "        self.output_layer = nn.Linear(observation_space * 2, action_space)\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, x):\n",
    "        #input states\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        #relu activation\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #actions\n",
    "        actions = self.output_layer(x)\n",
    "        \n",
    "        #get softmax for a probability distribution\n",
    "        action_probs = F.softmax(actions, dim=1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn state value\n",
    "class StateValueNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in state\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(observation_space, observation_space * 2)\n",
    "        self.output_layer = nn.Linear(observation_space * 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input layer\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        #activiation relu\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #get state value\n",
    "        state_value = self.output_layer(x)\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(network, state):\n",
    "    ''' Selects an action given current state\n",
    "    Args:\n",
    "    - network (Torch NN): network to process state\n",
    "    - state (Array): Array of action space in an environment\n",
    "    \n",
    "    Return:\n",
    "    - (int): action that is selected\n",
    "    - (float): log probability of selecting that action given state and network\n",
    "    '''\n",
    "    \n",
    "    #convert state to float tensor, add 1 dimension, allocate tensor on device\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    #use network to predict action probabilities\n",
    "    action_probs = network(state)\n",
    "    state = state.detach()\n",
    "    \n",
    "    #sample an action using the probability distribution\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    #return action\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rewards(rewards):\n",
    "    ''' Converts our rewards history into cumulative discounted rewards\n",
    "    Args:\n",
    "    - rewards (Array): array of rewards \n",
    "    \n",
    "    Returns:\n",
    "    - G (Array): array of cumulative discounted rewards\n",
    "    '''\n",
    "    #Calculate Gt (cumulative discounted rewards)\n",
    "    G = []\n",
    "    \n",
    "    #track cumulative reward\n",
    "    total_r = 0\n",
    "    \n",
    "    #iterate rewards from Gt to G0\n",
    "    for r in reversed(rewards):\n",
    "        \n",
    "        #Base case: G(T) = r(T)\n",
    "        #Recursive: G(t) = r(t) + G(t+1)^DISCOUNT\n",
    "        total_r = r + total_r * DISCOUNT_FACTOR\n",
    "        \n",
    "        #add to front of G\n",
    "        G.insert(0, total_r)\n",
    "    \n",
    "    #whitening rewards\n",
    "    G = torch.tensor(G).to(DEVICE)\n",
    "    G = (G - G.mean())/G.std()\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(deltas, log_probs, optimizer):\n",
    "    ''' Update policy parameters\n",
    "    Args:\n",
    "    - deltas (Array): difference between predicted stateval and actual stateval (Gt)\n",
    "    - log_probs (Array): trajectory of log probabilities of action taken\n",
    "    - optimizer (Pytorch optimizer): optimizer to update policy network parameters\n",
    "    '''\n",
    "    \n",
    "    #store updates\n",
    "    policy_loss = []\n",
    "    \n",
    "    #calculate loss to be backpropagated\n",
    "    for d, lp in zip(deltas, log_probs):\n",
    "        #add negative sign since we are performing gradient ascent\n",
    "        policy_loss.append(-d * lp)\n",
    "    \n",
    "    #Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    sum(policy_loss).backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value(G, state_vals, optimizer):\n",
    "    ''' Update state-value network parameters\n",
    "    Args:\n",
    "    - G (Array): trajectory of cumulative discounted rewards \n",
    "    - state_vals (Array): trajectory of predicted state-value at each step\n",
    "    - optimizer (Pytorch optimizer): optimizer to update state-value network parameters\n",
    "    '''\n",
    "    \n",
    "    #calculate MSE loss\n",
    "    val_loss = F.mse_loss(state_vals, G)\n",
    "        \n",
    "    #Backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    val_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = -0.001,\n",
    "    max_episode_steps = MAX_STEPS\n",
    ")\n",
    "\n",
    "OBS_SPACE = \"glyphs_crop\"\n",
    "\n",
    "observation_space_size = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "\n",
    "#Init network\n",
    "policy_network = PolicyNetwork(observation_space_size, env.action_space.n).to(DEVICE)\n",
    "stateval_network = StateValueNetwork(observation_space_size).to(DEVICE)\n",
    "\n",
    "\n",
    "#Init optimizer\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=1e-2)\n",
    "stateval_optimizer = optim.Adam(stateval_network.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#generate episode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MAX_STEPS):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m#env.render()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m#select action\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     action, lp \u001b[39m=\u001b[39m select_action(policy_network, state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#execute action\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     new_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb Cell 11\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(network, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m action \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msample()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#return action\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/REINFORCE_v3.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\u001b[39m.\u001b[39;49mitem(), m\u001b[39m.\u001b[39mlog_prob(action)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#track scores\n",
    "scores = []\n",
    "\n",
    "#recent 100 scores\n",
    "recent_scores = deque(maxlen=100)\n",
    "\n",
    "#iterate through episodes\n",
    "for episode in range(NUM_EPISODES):\n",
    "    \n",
    "    #reset environment, initiable variables\n",
    "    state = env.reset()\n",
    "    state = state[OBS_SPACE]\n",
    "    trajectory = []\n",
    "    score = 0\n",
    "    \n",
    "    #generate episode\n",
    "    for step in range(MAX_STEPS):\n",
    "        #env.render()\n",
    "        \n",
    "        #select action\n",
    "        action, lp = select_action(policy_network, state)\n",
    "        \n",
    "        #execute action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = new_state[OBS_SPACE]\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        #track episode score\n",
    "        score += reward\n",
    "        \n",
    "        #store into trajectory\n",
    "        trajectory.append([state, action, reward, lp])\n",
    "        \n",
    "        #end episode\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        #move into new state\n",
    "        state = new_state\n",
    "\n",
    "\n",
    "    if episode % 1 == 0:\n",
    "        print( f\"Episode: {episode}. Score: {score}\" )\n",
    "    \n",
    "    #append score\n",
    "    scores.append(score)\n",
    "    recent_scores.append(score)\n",
    "    \n",
    "    #check if agent finished training\n",
    "    if len(recent_scores) == 100:\n",
    "        average = sum(recent_scores)/len(recent_scores)\n",
    "        if average >= SOLVED_SCORE:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    #get items from trajectory\n",
    "    states = [step[0] for step in trajectory]\n",
    "    actions = [step[1] for step in trajectory]\n",
    "    rewards = [step[2] for step in trajectory]\n",
    "    lps = [step[3] for step in trajectory]\n",
    "    \n",
    "    #get discounted rewards\n",
    "    G = process_rewards(rewards)\n",
    "    #G = torch.tensor(G).to(DEVICE)\n",
    "    \n",
    "    #calculate state values and train statevalue network\n",
    "    state_vals = []\n",
    "    for state in states:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        state_vals.append(stateval_network(state))\n",
    "        \n",
    "    state_vals = torch.stack(state_vals).squeeze()\n",
    "    \n",
    "    train_value(G, state_vals, stateval_optimizer)\n",
    "        \n",
    "    #calculate deltas and train policy network\n",
    "    deltas = [gt - val for gt, val in zip(G, state_vals)]\n",
    "    deltas = torch.tensor(deltas).to(DEVICE)\n",
    "    \n",
    "    train_policy(deltas, lps, policy_optimizer)\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(scores).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
