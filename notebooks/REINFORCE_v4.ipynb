{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from environments.QuestEnvironment_v2 import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_steps = 5000\n",
    "alpha = 0.001\n",
    "alpha = 0.00001\n",
    "gamma = 0.9999\n",
    "epsilon = 1e-12\n",
    "epsilon = 1e-1\n",
    "OBS_SPACE = 'glyphs_crop'\n",
    "#OBS_SPACE = 'blstats'\n",
    "seed = 99\n",
    "verbose = True\n",
    "print_action_messages = False\n",
    "intermediate_plots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -100,\n",
    "    reward_win = 100,\n",
    "    penalty_step = -0.1,\n",
    "    penalty_time = -0.001,\n",
    "    max_episode_steps = max_steps,\n",
    "    seed = seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork:\n",
    "\n",
    "    def __init__(self, env, alpha):\n",
    "\n",
    "        if len(env.observation_space.spaces[OBS_SPACE].shape) == 1:\n",
    "            self.obs_space = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "        else:\n",
    "            self.obs_space = env.observation_space.spaces[OBS_SPACE].shape[0] * \\\n",
    "                        env.observation_space.spaces[OBS_SPACE].shape[1]\n",
    "\n",
    "        self.policy_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_space, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # hidden layers\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(self.obs_space*2, env.action_space.n),\n",
    "            torch.nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr = alpha)\n",
    "\n",
    "        self.value_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_space, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # hidden layers\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.obs_space, 1)\n",
    "        )\n",
    "\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr = alpha)\n",
    "\n",
    "\n",
    "nn = PolicyValueNetwork(env, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_counts = dict()\n",
    "coord_rewards = dict()\n",
    "\n",
    "def get_exploration_reward(state, reward):\n",
    "    s = state['blstats']\n",
    "\n",
    "    #return reward\n",
    "    coords = (int(s[0]), int(s[1]))\n",
    "\n",
    "    if coords not in visit_counts:\n",
    "        visit_counts[coords] = 1\n",
    "        #return 0.001\n",
    "        coord_rewards[coords] = reward\n",
    "    else:\n",
    "        #return 0\n",
    "        visit_counts[coords] += 1\n",
    "        coord_rewards[coords] += reward\n",
    "\n",
    "    return -0.001 * visit_counts[coords]\n",
    "\n",
    "    r = 0\n",
    "    r += coord_rewards[coords] / visit_counts[coords]\n",
    "    r += 0.01 * math.sqrt( \\\n",
    "        (math.log(visit_counts[coords])) \\\n",
    "        / \\\n",
    "        visit_counts[coords])\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    obs = obs[OBS_SPACE]\n",
    "    obs = torch.tensor(obs, dtype=torch.float)  \n",
    "    obs = torch.flatten(obs)\n",
    "    obs = torch.reshape(obs, (1, obs.shape[0]))\n",
    "    obs = torch.nn.functional.normalize(obs, p=2.0, dim=1, eps=epsilon, out=None)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_text = [\n",
    "    'EAST', 'NORTH', 'SOUTH', 'WEST', \n",
    "    #'UP', \n",
    "    'DOWN', \n",
    "    #'OPEN', \n",
    "    #'MOVE',\n",
    "    'EAT',\n",
    "    'PICKUP',\n",
    "    'ZAP', \n",
    "    'APPLY'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training\n",
    "subproblem_counts = np.zeros(4)\n",
    "save_video = False\n",
    "previous_description = ''\n",
    "messages_received = dict()\n",
    "actions_taken = dict()\n",
    "\n",
    "TotalRewards = []\n",
    "TotalLoss = []\n",
    "Total_Rewards = []\n",
    "\n",
    "for k in range(num_episodes):\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0\n",
    "\n",
    "    obs = env.reset(save_video)\n",
    "\n",
    "    save_video = False\n",
    "    \n",
    "    done = False\n",
    "    Actions, States, Rewards = [], [], []\n",
    "\n",
    "    for h in range(max_steps):\n",
    "\n",
    "        message = obs['message']\n",
    "        msg = bytes(message)\n",
    "        msg = str(msg).replace(\"'\", \"\").replace('\"', '').lstrip(\"b\").rstrip(\"\\\\x00\").rstrip(\"\\x00\")\n",
    "\n",
    "        if msg not in messages_received:\n",
    "            messages_received[msg] = 1\n",
    "        else:\n",
    "            messages_received[msg] += 1\n",
    "\n",
    "        obs = convert_observation(obs)\n",
    "\n",
    "        probs = nn.policy_net(obs)\n",
    "        dist = torch.distributions.Categorical(probs = probs)\n",
    "        action = dist.sample().item()\n",
    "\n",
    "        obs_, rew, done, info = env.step(action)\n",
    "\n",
    "        stats = obs_['blstats']\n",
    "        coords = (stats[0], stats[1])\n",
    "\n",
    "        if coords == (27, 11):\n",
    "            subproblem_counts[0] += 1\n",
    "\n",
    "        if action_text[action] not in actions_taken:\n",
    "            actions_taken[action_text[action]] = 1\n",
    "        else:\n",
    "            actions_taken[action_text[action]] += 1\n",
    "\n",
    "        if verbose:\n",
    "            #print( f\"\\rEpisode: {k+1}, Step: {h+1}, Coords: {coords}, Reward: {rew:0.4f}, Action: {action_text[action]}  \n",
    "            print( f\"\\rEpisode: {k+1}, Steps: {h+1}        \", end=\"\")\n",
    "\n",
    "        rew = rew + get_exploration_reward(obs_, rew)\n",
    "\n",
    "        if rew > 5:\n",
    "            save_video = True\n",
    "        \n",
    "        #env.render()\n",
    "\n",
    "        episode_reward += rew\n",
    "\n",
    "        Actions.append(torch.tensor(action, dtype=torch.int))\n",
    "        States.append(obs)\n",
    "        Rewards.append(rew)\n",
    "\n",
    "        # move to the next state\n",
    "        obs = obs_\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    TotalRewards.append(episode_reward)\n",
    "\n",
    "    DiscountedReturns = []\n",
    "    r = 0\n",
    "    # discount over the trajectory\n",
    "    for step in reversed(range(len(Rewards))):\n",
    "        r = Rewards[step] + gamma * r\n",
    "        DiscountedReturns.insert(0, r)\n",
    "\n",
    "    # normalize the returns?\n",
    "    # ensure type \n",
    "    DiscountedReturns = np.array(DiscountedReturns)\n",
    "\n",
    "    # calculate the mean and std\n",
    "    mean = DiscountedReturns.mean(axis = 0)\n",
    "    std = DiscountedReturns.std(axis = 0)\n",
    "\n",
    "    # average and normalize the returns\n",
    "    if std != 0.0:\n",
    "        DiscountedReturns = (DiscountedReturns - mean) / std\n",
    "\n",
    "    PolicyLoss = []\n",
    "    ValueLoss = []\n",
    "    Total_Loss = []\n",
    "\n",
    "    sum_loss = 0\n",
    "    \n",
    "    for State, Action, G in zip(States, Actions, DiscountedReturns):\n",
    "\n",
    "        value = nn.value_net(State)\n",
    "\n",
    "        delta = G - value\n",
    "\n",
    "        probs = nn.policy_net(State)\n",
    "        policy_dist = torch.distributions.Categorical(probs = probs)\n",
    "        log_probs = policy_dist.log_prob(Action)\n",
    "\n",
    "        policy_loss = -( delta.detach() * log_probs )\n",
    "        nn.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        nn.policy_optimizer.step()\n",
    "\n",
    "        pl = policy_loss.detach().numpy()\n",
    "        PolicyLoss.append(pl)\n",
    "\n",
    "        value_loss = torch.sum(delta**2) / 2\n",
    "        nn.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.value_optimizer.step()\n",
    "\n",
    "        vl = value_loss.detach().numpy()\n",
    "        ValueLoss.append(vl)\n",
    "        \n",
    "        # probs = nn(State)\n",
    "        # dist = torch.distributions.Categorical(probs=probs)    \n",
    "        # log_prob = dist.log_prob(Action)\n",
    "        \n",
    "        # loss = -(log_prob*G)\n",
    "        total_loss = pl + vl\n",
    "        sum_loss += total_loss[0]\n",
    "        Total_Loss.append(total_loss)\n",
    "        \n",
    "        # optim.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optim.step()\n",
    "\n",
    "    TotalLoss.append(sum_loss)\n",
    "\n",
    "    if verbose:\n",
    "        print( f\"\"\"\n",
    "            Tot. Rewards: {np.sum(Rewards):0.4f}, Avg. Reward: {np.mean(Rewards):0.4f}, \n",
    "            Tot. Loss: {np.sum(Total_Loss):0.4f}, Avg Loss: {np.mean(Total_Loss):0.4f},\n",
    "            Tot. Policy Loss: {np.sum(PolicyLoss):0.4f}, Avg Policy Loss: {np.mean(PolicyLoss):0.4f},\n",
    "            Tot. Value Loss: {np.sum(ValueLoss):0.4f}, Avg Value Loss: {np.mean(ValueLoss):0.4f}\n",
    "            \"\"\")\n",
    "\n",
    "    if print_action_messages:\n",
    "\n",
    "        print(\"\\nActions Taken:\")\n",
    "        for key, val in actions_taken.items():\n",
    "            print(f\"- {key}: {val}\")\n",
    "\n",
    "        print(\"\\nMessages Received:\")\n",
    "        for key, val in messages_received.items():\n",
    "            print(f\"- {key}: {val}\")\n",
    "\n",
    "    if intermediate_plots:\n",
    "\n",
    "        PolicyLoss = np.reshape(PolicyLoss, (np.shape(PolicyLoss)[0], 1))\n",
    "        ValueLoss = np.reshape(ValueLoss, (np.shape(ValueLoss)[0], 1))\n",
    "        Total_Loss = np.reshape(Total_Loss, (np.shape(Total_Loss)[0], 1))\n",
    "\n",
    "        #Plot stuff\n",
    "        window = int(max_steps * 0.01)\n",
    "        fig, axs = plt.subplots(3, 2)\n",
    "        fig.set_figheight(6)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "        #axs[0].plot(pd.DataFrame(Rewards))\n",
    "\n",
    "        axs[0][0].title.set_text('Reward')\n",
    "        #axs[0][0].set_xlabel('episodes')\n",
    "        axs[0][0].set_ylabel('reward')\n",
    "        axs[0][0].plot(pd.DataFrame(Rewards))\n",
    "        axs[0][0].plot(pd.DataFrame(Rewards).rolling(window).mean())\n",
    "        axs[0][0].grid(True)\n",
    "\n",
    "        axs[0][1].title.set_text('Total Loss')\n",
    "        #axs[0][1].set_xlabel('episodes')\n",
    "        axs[0][1].set_ylabel('loss')\n",
    "        axs[0][1].plot(pd.DataFrame(Total_Loss))\n",
    "        axs[0][1].plot(pd.DataFrame(Total_Loss).rolling(window).mean())\n",
    "        axs[0][1].grid(True)\n",
    "\n",
    "        axs[1][0].title.set_text('Policy Loss')\n",
    "        axs[1][0].set_xlabel('episodes')\n",
    "        axs[1][0].set_ylabel('loss')\n",
    "        axs[1][0].plot(pd.DataFrame(PolicyLoss))\n",
    "        axs[1][0].plot(pd.DataFrame(PolicyLoss).rolling(window).mean())\n",
    "        axs[1][0].grid(True)\n",
    "\n",
    "        axs[1][1].title.set_text('Value Loss')\n",
    "        axs[1][1].set_xlabel('episodes')\n",
    "        axs[1][1].set_ylabel('loss')\n",
    "        axs[1][1].plot(pd.DataFrame(ValueLoss))\n",
    "        axs[1][1].plot(pd.DataFrame(ValueLoss).rolling(window).mean())\n",
    "        axs[1][1].grid(True)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = int(max_steps * 0.01)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "axs[0].title.set_text('Total Reward')\n",
    "axs[0].set_xlabel('episodes')\n",
    "axs[0].set_ylabel('reward')\n",
    "axs[0].plot(pd.DataFrame(Rewards))\n",
    "axs[0].plot(pd.DataFrame(Rewards).rolling(window).mean())\n",
    "\n",
    "axs[1].title.set_text('Total Loss')\n",
    "axs[1].set_xlabel('episodes')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].plot(pd.DataFrame(TotalLoss))\n",
    "axs[1].plot(pd.DataFrame(TotalLoss).rolling(window).mean())\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "subtasks = [\"Maze Exit\",\"Crossed lava river\",\"Defeated Demon\",\"Staircase\"]\n",
    "# creating the bar plot\n",
    "plt.bar(subtasks, subproblem_counts, color ='maroon',\n",
    "        width = 0.4)\n",
    "plt.ylim(0, 100)\n",
    "plt.xlabel(\"Sub tasks\")\n",
    "plt.ylabel(\"Times Completed\")\n",
    "plt.title(\"Number of Times Subtasks have been completed per 100 episodes\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
