{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_steps = 5000\n",
    "alpha = 0.001\n",
    "gamma = 0.9999\n",
    "epsilon = 1e-12\n",
    "epsilon = 1e-1\n",
    "OBS_SPACE = 'glyphs_crop'\n",
    "#OBS_SPACE = 'blstats'\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -10,\n",
    "    reward_win = 10,\n",
    "    penalty_step = -0.002,\n",
    "    penalty_time = -0.002,\n",
    "    max_episode_steps = max_steps,\n",
    "    seed = seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork:\n",
    "\n",
    "    def __init__(self, env, alpha):\n",
    "\n",
    "        if len(env.observation_space.spaces[OBS_SPACE].shape) == 1:\n",
    "            self.obs_space = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "        else:\n",
    "            self.obs_space = env.observation_space.spaces[OBS_SPACE].shape[0] * \\\n",
    "                        env.observation_space.spaces[OBS_SPACE].shape[1]\n",
    "\n",
    "        self.policy_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_space, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.obs_space*2, env.action_space.n),\n",
    "            torch.nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr = alpha)\n",
    "\n",
    "        self.value_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_space, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.obs_space, 1)\n",
    "        )\n",
    "\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr = alpha)\n",
    "\n",
    "\n",
    "nn = PolicyValueNetwork(env, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(env.observation_space.spaces[OBS_SPACE].shape) == 1:\n",
    "#     obs_space = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "# else:\n",
    "#     obs_space = env.observation_space.spaces[OBS_SPACE].shape[0] * \\\n",
    "#                 env.observation_space.spaces[OBS_SPACE].shape[1]\n",
    "\n",
    "# nn = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(obs_space, obs_space*2),\n",
    "#     torch.nn.ReLU(),\n",
    "\n",
    "#     # hidden layers\n",
    "#     #torch.nn.Linear(obs_space*2, obs_space*2),\n",
    "#     #torch.nn.ReLU(),\n",
    "#     # torch.nn.Linear(obs_space*3, obs_space*2),\n",
    "#     # torch.nn.ReLU(),\n",
    "\n",
    "#     torch.nn.Linear(obs_space*2, env.action_space.n),\n",
    "#     torch.nn.Softmax(dim = -1)\n",
    "# )\n",
    "# optim = torch.optim.Adam(nn.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_counts = dict()\n",
    "coord_rewards = dict()\n",
    "\n",
    "def get_exploration_reward(state, reward):\n",
    "    s = state['blstats']\n",
    "\n",
    "    #return reward\n",
    "    coords = (int(s[0]), int(s[1]))\n",
    "\n",
    "    if coords not in visit_counts:\n",
    "        visit_counts[coords] = 1\n",
    "        #return 0.001\n",
    "        coord_rewards[coords] = reward\n",
    "    else:\n",
    "        #return 0\n",
    "        visit_counts[coords] += 1\n",
    "        coord_rewards[coords] += reward\n",
    "\n",
    "    r = 0\n",
    "    r += coord_rewards[coords] / visit_counts[coords]\n",
    "    r += 0.01 * math.sqrt( \\\n",
    "        (math.log(visit_counts[coords])) \\\n",
    "        / \\\n",
    "        visit_counts[coords])\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    obs = obs[OBS_SPACE]\n",
    "    obs = torch.tensor(obs, dtype=torch.float)  \n",
    "    obs = torch.flatten(obs)\n",
    "    obs = torch.reshape(obs, (1, obs.shape[0]))\n",
    "    obs = torch.nn.functional.normalize(obs, p=2.0, dim=1, eps=epsilon, out=None)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "for k in range(num_episodes):\n",
    "\n",
    "    #visit_counts = dict()\n",
    "    #coord_rewards = dict()\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = convert_observation(obs)\n",
    "    \n",
    "    done = False\n",
    "    Actions, States, Rewards = [], [], []\n",
    "\n",
    "    for h in range(max_steps):\n",
    "\n",
    "        probs = nn.policy_net(obs)\n",
    "        dist = torch.distributions.Categorical(probs = probs)\n",
    "        action = dist.sample().item()\n",
    "\n",
    "        # probs = nn(obs)\n",
    "        # #dist = probs.detach().numpy()\n",
    "        # #print(dist)\n",
    "        # # if 0.1 > np.random.uniform():\n",
    "        # #     action = np.random.choice([*range(env.action_space.n)])\n",
    "        # # else:\n",
    "        # dist = torch.distributions.Categorical(probs=probs)        \n",
    "        # action = dist.sample().item()\n",
    "        \n",
    "        print( f\"\\rEpisode: {k+1}, Step: {h+1}, Action: {action}\", end=\"\")\n",
    "\n",
    "        obs_, rew, done, _ = env.step(action)\n",
    "        rew = rew + get_exploration_reward(obs_, rew)\n",
    "        obs_ = convert_observation(obs_)\n",
    "        env.render()\n",
    "        \n",
    "        Actions.append(torch.tensor(action, dtype=torch.int))\n",
    "        States.append(obs)\n",
    "        Rewards.append(rew)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # DiscountedReturns = []\n",
    "    # for t in range(len(Rewards)):\n",
    "    #     G = 0.0\n",
    "    #     for i, r in enumerate(Rewards[t:]):\n",
    "    #         G += (gamma**i)*r\n",
    "    #     DiscountedReturns.append(G)\n",
    "\n",
    "    DiscountedReturns = []\n",
    "    r = 0\n",
    "    # discount over the trajectory\n",
    "    for step in reversed(range(len(Rewards))):\n",
    "        r = Rewards[step] + gamma * r\n",
    "        DiscountedReturns.insert(0, r)\n",
    "\n",
    "    # normalize the returns?\n",
    "    # ensure type \n",
    "    DiscountedReturns = np.array(DiscountedReturns)\n",
    "\n",
    "    # calculate the mean and std\n",
    "    mean = DiscountedReturns.mean(axis = 0)\n",
    "    std = DiscountedReturns.std(axis = 0)\n",
    "\n",
    "    # average and normalize the returns\n",
    "    if std == 0.0:\n",
    "        DiscountedReturns = 0.0\n",
    "    else:\n",
    "        DiscountedReturns = (DiscountedReturns - mean) / std\n",
    "\n",
    "    Loss = []\n",
    "    \n",
    "    for State, Action, G in zip(States, Actions, DiscountedReturns):\n",
    "\n",
    "        value = nn.value_net(State)\n",
    "\n",
    "        delta = G - value\n",
    "\n",
    "        probs = nn.policy_net(State)\n",
    "        policy_dist = torch.distributions.Categorical(probs = probs)\n",
    "        log_probs = policy_dist.log_prob(Action)\n",
    "\n",
    "        policy_loss = -( delta.detach() * log_probs )\n",
    "        nn.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        nn.policy_optimizer.step()\n",
    "\n",
    "        value_loss = torch.sum(delta**2) / 2\n",
    "        nn.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.value_optimizer.step()\n",
    "        \n",
    "        # probs = nn(State)\n",
    "        # dist = torch.distributions.Categorical(probs=probs)    \n",
    "        # log_prob = dist.log_prob(Action)\n",
    "        \n",
    "        # loss = -(log_prob*G)\n",
    "        total_loss = policy_loss + value_loss\n",
    "        Loss.append(total_loss.detach().numpy())\n",
    "        \n",
    "        # optim.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optim.step()\n",
    "\n",
    "    print( f\", Tot. Rewards: {np.sum(Rewards):0.4f}, Avg. Reward: {np.mean(Rewards):0.4f}, Tot. Loss: {np.sum(Loss):0.4f}, Avg Loss: {np.mean(Loss):0.4f}\")\n",
    "\n",
    "    Loss = np.reshape(Loss, (np.shape(Loss)[0], 1))\n",
    "\n",
    "    #Plot stuff\n",
    "    window = int(max_steps * 0.01)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Reward')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('reward')\n",
    "    plt.plot(pd.DataFrame(Rewards))\n",
    "    plt.plot(pd.DataFrame(Rewards).rolling(window).mean())\n",
    "    plt.grid(True)\n",
    "    plt.subplot(122)\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(pd.DataFrame(Loss))\n",
    "    plt.plot(pd.DataFrame(Loss).rolling(window).mean())\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "\n",
    "    Rewards = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = convert_observation(obs)\n",
    "\n",
    "    done = False\n",
    "    env.render()\n",
    "\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps <= max_steps:\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        probs = nn(obs)\n",
    "\n",
    "        c = torch.distributions.Categorical(probs=probs)        \n",
    "        action = c.sample().item()\n",
    "        \n",
    "        obs_, rew, done, _info = env.step(action)\n",
    "        obs_ = convert_observation(obs_)\n",
    "        env.render()\n",
    "\n",
    "        Rewards.append(rew)\n",
    "    \n",
    "\n",
    "    print(f'Reward: {sum(Rewards)}')\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
