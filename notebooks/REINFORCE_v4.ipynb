{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# resolve path for notebook\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import pyglet\n",
    "import random\n",
    "import minihack\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nle import nethack\n",
    "from collections import deque\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from environments.QuestEnvironment import QuestEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a Cuda GPU, then we want to use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_steps = 5000\n",
    "alpha = 0.001\n",
    "alpha = 0.00001\n",
    "gamma = 0.9999\n",
    "epsilon = 1e-12\n",
    "epsilon = 1e-1\n",
    "OBS_SPACE = 'glyphs_crop'\n",
    "#OBS_SPACE = 'blstats'\n",
    "seed = 99\n",
    "verbose = True\n",
    "intermediate_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuestEnvironment().create(\n",
    "    reward_lose = -100,\n",
    "    reward_win = 100,\n",
    "    penalty_step = -0.01,\n",
    "    penalty_time = 0,\n",
    "    max_episode_steps = max_steps,\n",
    "    seed = seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyValueNetwork:\n",
    "\n",
    "    def __init__(self, env, alpha):\n",
    "\n",
    "        if len(env.observation_space.spaces[OBS_SPACE].shape) == 1:\n",
    "            self.obs_space = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "        else:\n",
    "            self.obs_space = env.observation_space.spaces[OBS_SPACE].shape[0] * \\\n",
    "                        env.observation_space.spaces[OBS_SPACE].shape[1]\n",
    "\n",
    "        self.policy_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_space, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # hidden layers\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(self.obs_space*2, env.action_space.n),\n",
    "            torch.nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr = alpha)\n",
    "\n",
    "        self.value_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.obs_space, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # hidden layers\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space*2),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(self.obs_space*2, self.obs_space),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.obs_space, 1)\n",
    "        )\n",
    "\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr = alpha)\n",
    "\n",
    "\n",
    "nn = PolicyValueNetwork(env, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(env.observation_space.spaces[OBS_SPACE].shape) == 1:\n",
    "#     obs_space = env.observation_space.spaces[OBS_SPACE].shape[0]\n",
    "# else:\n",
    "#     obs_space = env.observation_space.spaces[OBS_SPACE].shape[0] * \\\n",
    "#                 env.observation_space.spaces[OBS_SPACE].shape[1]\n",
    "\n",
    "# nn = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(obs_space, obs_space*2),\n",
    "#     torch.nn.ReLU(),\n",
    "\n",
    "#     # hidden layers\n",
    "#     #torch.nn.Linear(obs_space*2, obs_space*2),\n",
    "#     #torch.nn.ReLU(),\n",
    "#     # torch.nn.Linear(obs_space*3, obs_space*2),\n",
    "#     # torch.nn.ReLU(),\n",
    "\n",
    "#     torch.nn.Linear(obs_space*2, env.action_space.n),\n",
    "#     torch.nn.Softmax(dim = -1)\n",
    "# )\n",
    "# optim = torch.optim.Adam(nn.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_counts = dict()\n",
    "coord_rewards = dict()\n",
    "\n",
    "def get_exploration_reward(state, reward):\n",
    "    s = state['blstats']\n",
    "\n",
    "    #return reward\n",
    "    coords = (int(s[0]), int(s[1]))\n",
    "\n",
    "    if coords not in visit_counts:\n",
    "        visit_counts[coords] = 1\n",
    "        #return 0.001\n",
    "        coord_rewards[coords] = reward\n",
    "    else:\n",
    "        #return 0\n",
    "        visit_counts[coords] += 1\n",
    "        coord_rewards[coords] += reward\n",
    "\n",
    "    r = 0\n",
    "    r += coord_rewards[coords] / visit_counts[coords]\n",
    "    r += 0.01 * math.sqrt( \\\n",
    "        (math.log(visit_counts[coords])) \\\n",
    "        / \\\n",
    "        visit_counts[coords])\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    obs = obs[OBS_SPACE]\n",
    "    obs = torch.tensor(obs, dtype=torch.float)  \n",
    "    obs = torch.flatten(obs)\n",
    "    obs = torch.reshape(obs, (1, obs.shape[0]))\n",
    "    obs = torch.nn.functional.normalize(obs, p=2.0, dim=1, eps=epsilon, out=None)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_text = [\n",
    "    'EAST', 'NORTH', 'SOUTH', 'WEST', \n",
    "    #'UP', \n",
    "    'DOWN', \n",
    "    #'OPEN', \n",
    "    #'MOVE',\n",
    "    'EAT'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training\n",
    "previous_description = ''\n",
    "\n",
    "for k in range(num_episodes):\n",
    "\n",
    "    #visit_counts = dict()\n",
    "    #coord_rewards = dict()\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    Actions, States, Rewards = [], [], []\n",
    "\n",
    "    for h in range(max_steps):\n",
    "\n",
    "        if verbose:\n",
    "            message = obs['message']\n",
    "            msg = bytes(message)\n",
    "            print(msg[: msg.index(b\"\\0\")])\n",
    "\n",
    "        obs = convert_observation(obs)\n",
    "\n",
    "        probs = nn.policy_net(obs)\n",
    "        dist = torch.distributions.Categorical(probs = probs)\n",
    "        action = dist.sample().item()\n",
    "\n",
    "        # probs = nn(obs)\n",
    "        # #dist = probs.detach().numpy()\n",
    "        # #print(dist)\n",
    "        # # if 0.1 > np.random.uniform():\n",
    "        # #     action = np.random.choice([*range(env.action_space.n)])\n",
    "        # # else:\n",
    "        # dist = torch.distributions.Categorical(probs=probs)        \n",
    "        # action = dist.sample().item()\n",
    "\n",
    "        obs_, rew, done, _ = env.step(action)\n",
    "\n",
    "        stats = obs_['blstats']\n",
    "        coords = (stats[0], stats[1])\n",
    "\n",
    "        # for i in range(-1, 2):\n",
    "        #     for j in range(-1, 2):\n",
    "        #         d = env.unwrapped.get_screen_description(coords[0] + i, coords[1] + j)\n",
    "        #         if d != 'corridor': print( f\", {d}\")\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print( f\"\\rEpisode: {k+1}, Step: {h+1}, Coords: {coords}, Reward: {rew:0.4f}, Action: {action_text[action]}          \", end=\"\")\n",
    "        \n",
    "        #rew = rew + get_exploration_reward(obs_, rew)\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        Actions.append(torch.tensor(action, dtype=torch.int))\n",
    "        States.append(obs)\n",
    "        Rewards.append(rew)\n",
    "\n",
    "        # move to the next state\n",
    "        obs = obs_\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # DiscountedReturns = []\n",
    "    # for t in range(len(Rewards)):\n",
    "    #     G = 0.0\n",
    "    #     for i, r in enumerate(Rewards[t:]):\n",
    "    #         G += (gamma**i)*r\n",
    "    #     DiscountedReturns.append(G)\n",
    "\n",
    "    DiscountedReturns = []\n",
    "    r = 0\n",
    "    # discount over the trajectory\n",
    "    for step in reversed(range(len(Rewards))):\n",
    "        r = Rewards[step] + gamma * r\n",
    "        DiscountedReturns.insert(0, r)\n",
    "\n",
    "    # normalize the returns?\n",
    "    # ensure type \n",
    "    DiscountedReturns = np.array(DiscountedReturns)\n",
    "\n",
    "    # calculate the mean and std\n",
    "    mean = DiscountedReturns.mean(axis = 0)\n",
    "    std = DiscountedReturns.std(axis = 0)\n",
    "\n",
    "    # average and normalize the returns\n",
    "    if std != 0.0:\n",
    "        DiscountedReturns = (DiscountedReturns - mean) / std\n",
    "\n",
    "    PolicyLoss = []\n",
    "    ValueLoss = []\n",
    "    TotalLoss = []\n",
    "    \n",
    "    for State, Action, G in zip(States, Actions, DiscountedReturns):\n",
    "\n",
    "        value = nn.value_net(State)\n",
    "\n",
    "        delta = G - value\n",
    "\n",
    "        probs = nn.policy_net(State)\n",
    "        policy_dist = torch.distributions.Categorical(probs = probs)\n",
    "        log_probs = policy_dist.log_prob(Action)\n",
    "\n",
    "        policy_loss = -( delta.detach() * log_probs )\n",
    "        nn.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        nn.policy_optimizer.step()\n",
    "\n",
    "        pl = policy_loss.detach().numpy()\n",
    "        PolicyLoss.append(pl)\n",
    "\n",
    "        value_loss = torch.sum(delta**2) / 2\n",
    "        nn.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.value_optimizer.step()\n",
    "\n",
    "        vl = value_loss.detach().numpy()\n",
    "        ValueLoss.append(vl)\n",
    "        \n",
    "        # probs = nn(State)\n",
    "        # dist = torch.distributions.Categorical(probs=probs)    \n",
    "        # log_prob = dist.log_prob(Action)\n",
    "        \n",
    "        # loss = -(log_prob*G)\n",
    "        total_loss = pl + vl\n",
    "        TotalLoss.append(total_loss)\n",
    "        \n",
    "        # optim.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optim.step()\n",
    "\n",
    "    if verbose:\n",
    "        print( f\"\"\"\n",
    "            Tot. Rewards: {np.sum(Rewards):0.4f}, Avg. Reward: {np.mean(Rewards):0.4f}, \n",
    "            Tot. Loss: {np.sum(TotalLoss):0.4f}, Avg Loss: {np.mean(TotalLoss):0.4f},\n",
    "            Tot. Policy Loss: {np.sum(PolicyLoss):0.4f}, Avg Policy Loss: {np.mean(PolicyLoss):0.4f},\n",
    "            Tot. Value Loss: {np.sum(ValueLoss):0.4f}, Avg Value Loss: {np.mean(ValueLoss):0.4f}\n",
    "            \"\"\")\n",
    "\n",
    "    if intermediate_plots:\n",
    "\n",
    "        PolicyLoss = np.reshape(PolicyLoss, (np.shape(PolicyLoss)[0], 1))\n",
    "        ValueLoss = np.reshape(ValueLoss, (np.shape(ValueLoss)[0], 1))\n",
    "        TotalLoss = np.reshape(TotalLoss, (np.shape(ValueLoss)[0], 1))\n",
    "\n",
    "        #Plot stuff\n",
    "        window = int(max_steps * 0.01)\n",
    "        fig, axs = plt.subplots(2, 2)\n",
    "        fig.set_figheight(6)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "        #axs[0].plot(pd.DataFrame(Rewards))\n",
    "\n",
    "        axs[0][0].title.set_text('Reward')\n",
    "        #axs[0][0].set_xlabel('episodes')\n",
    "        axs[0][0].set_ylabel('reward')\n",
    "        axs[0][0].plot(pd.DataFrame(Rewards))\n",
    "        axs[0][0].plot(pd.DataFrame(Rewards).rolling(window).mean())\n",
    "        axs[0][0].grid(True)\n",
    "\n",
    "        axs[0][1].title.set_text('Total Loss')\n",
    "        #axs[0][1].set_xlabel('episodes')\n",
    "        axs[0][1].set_ylabel('loss')\n",
    "        axs[0][1].plot(pd.DataFrame(TotalLoss))\n",
    "        axs[0][1].plot(pd.DataFrame(TotalLoss).rolling(window).mean())\n",
    "        axs[0][1].grid(True)\n",
    "\n",
    "        axs[1][0].title.set_text('Policy Loss')\n",
    "        axs[1][0].set_xlabel('episodes')\n",
    "        axs[1][0].set_ylabel('loss')\n",
    "        axs[1][0].plot(pd.DataFrame(PolicyLoss))\n",
    "        axs[1][0].plot(pd.DataFrame(PolicyLoss).rolling(window).mean())\n",
    "        axs[1][0].grid(True)\n",
    "\n",
    "        axs[1][1].title.set_text('Value Loss')\n",
    "        axs[1][1].set_xlabel('episodes')\n",
    "        axs[1][1].set_ylabel('loss')\n",
    "        axs[1][1].plot(pd.DataFrame(ValueLoss))\n",
    "        axs[1][1].plot(pd.DataFrame(ValueLoss).rolling(window).mean())\n",
    "        axs[1][1].grid(True)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
