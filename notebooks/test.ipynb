{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:\t1.13.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "print(\"PyTorch:\\t{}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: {} Average of last 100:8.00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/n99nr9kx3fd5d975tlfwmh6r0000gn/T/ipykernel_10755/1186764614.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  state_tensor = torch.FloatTensor(batch_states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: {} Average of last 100:11.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m policy_est \u001b[39m=\u001b[39m policy_estimator(env)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m rewards \u001b[39m=\u001b[39m reinforce(env, policy_est)\n",
      "\u001b[1;32m/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb Cell 2\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(env, policy_estimator, num_episodes, batch_size, gamma)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mEp: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Average of last 100:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m                  ep \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, avg_rewards), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m             ep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m             env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gltch/Documents/code/github/RL_Project/notebooks/test.ipynb#W1sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mreturn\u001b[39;00m total_rewards\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:229\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcarttrans\u001b[39m.\u001b[39mset_translation(cartx, carty)\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoletrans\u001b[39m.\u001b[39mset_rotation(\u001b[39m-\u001b[39mx[\u001b[39m2\u001b[39m])\n\u001b[0;32m--> 229\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviewer\u001b[39m.\u001b[39;49mrender(return_rgb_array\u001b[39m=\u001b[39;49mmode \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py:145\u001b[0m, in \u001b[0;36mViewer.render\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    143\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mreshape(buffer\u001b[39m.\u001b[39mheight, buffer\u001b[39m.\u001b[39mwidth, \u001b[39m4\u001b[39m)\n\u001b[1;32m    144\u001b[0m     arr \u001b[39m=\u001b[39m arr[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, \u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[0;32m--> 145\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow\u001b[39m.\u001b[39;49mflip()\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monetime_geoms \u001b[39m=\u001b[39m []\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m arr \u001b[39mif\u001b[39;00m return_rgb_array \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misopen\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/pyglet/window/cocoa/__init__.py:296\u001b[0m, in \u001b[0;36mCocoaWindow.flip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdraw_mouse_cursor()\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mflip()\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/pyglet/gl/cocoa.py:335\u001b[0m, in \u001b[0;36mCocoaContext.flip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflip\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nscontext\u001b[39m.\u001b[39;49mflushBuffer()\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py:805\u001b[0m, in \u001b[0;36mObjCBoundMethod.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    804\u001b[0m     \u001b[39m\"\"\"Call the method with the given arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmethod(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjc_id, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Documents/code/github/RL_Project/.cenv/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py:775\u001b[0m, in \u001b[0;36mObjCMethod.__call__\u001b[0;34m(self, objc_id, *args)\u001b[0m\n\u001b[1;32m    773\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_callable()\n\u001b[1;32m    774\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 775\u001b[0m     result \u001b[39m=\u001b[39m f(objc_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselector, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    776\u001b[0m     \u001b[39m# Convert result to python type if it is a instance or class pointer.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestype \u001b[39m==\u001b[39m ObjCInstance:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class policy_estimator():\n",
    "    def __init__(self, env):\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state))\n",
    "        return action_probs\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "        for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    return r - r.mean()\n",
    "\n",
    "def reinforce(env, policy_estimator, num_episodes=2000,\n",
    "              batch_size=10, gamma=0.99):\n",
    "    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), \n",
    "                           lr=0.01)\n",
    "    \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    ep = 0\n",
    "    while ep < num_episodes:\n",
    "        s_0 = env.reset()\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        done = False\n",
    "        while done == False:\n",
    "            # Get actions and convert to numpy array\n",
    "            action_probs = policy_estimator.predict(\n",
    "                s_0).detach().numpy()\n",
    "            action = np.random.choice(action_space, \n",
    "                p=action_probs)\n",
    "            s_1, r, done, _ = env.step(action)\n",
    "            \n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "            \n",
    "            # If done, batch data\n",
    "            if done:\n",
    "                batch_rewards.extend(discount_rewards(\n",
    "                    rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "                \n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(\n",
    "                        batch_rewards)\n",
    "                    # Actions are used as indices, must be \n",
    "                    # LongTensor\n",
    "                    action_tensor = torch.LongTensor(\n",
    "                       batch_actions)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    pred = policy_estimator.predict(state_tensor)\n",
    "                    logprob = torch.log(pred)\n",
    "                    selected_logprobs = reward_tensor * torch.gather(logprob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
    "                    loss = -selected_logprobs.mean()\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "                    \n",
    "                avg_rewards = np.mean(total_rewards[-100:])\n",
    "                # Print running average\n",
    "                print(\"\\rEp: {} Average of last 100:\" +   \n",
    "                     \"{:.2f}\".format(\n",
    "                     ep + 1, avg_rewards), end=\"\")\n",
    "                ep += 1\n",
    "                env.render()\n",
    "                \n",
    "    return total_rewards\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "policy_est = policy_estimator(env)\n",
    "rewards = reinforce(env, policy_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47c508eda2f135ead275feb6da5dfe27515f3d548c4a43b869658eb5e3bf749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
