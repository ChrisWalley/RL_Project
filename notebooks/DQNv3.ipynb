{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEIQMVB2Dp3M"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNE3lZ4Xugz-"
      },
      "source": [
        "Intstallations required in order to run the code on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJOIk2fyK-km",
        "outputId": "ddc2edf9-5360-40f7-8926-ed4a179cd613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "autoconf is already the newest version (2.69-11).\n",
            "bison is already the newest version (2:3.0.4.dfsg-1build1).\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "flex is already the newest version (2.6.4-6).\n",
            "libtool is already the newest version (2.4.6-2).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "python3-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.13).\n",
            "libncurses5-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "libzmq3-dev is already the newest version (4.2.5-1ubuntu0.2).\n",
            "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.5).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "fatal: destination path 'flatbuffers' already exists and is not an empty directory.\n",
            "CMake Warning:\n",
            "  No source or binary directory provided.  Both will be assumed to be the\n",
            "  same as the current working directory, but note that this warning will\n",
            "  become a fatal error in future CMake releases.\n",
            "\n",
            "\n",
            "CMake Warning at CMakeLists.txt:8 (message):\n",
            "  Using cmake version 3.15.3 which is older than our target version of 3.16.\n",
            "  This will use the legacy CMakeLists.txt that supports version 2.8.12 and\n",
            "  higher, but not actively maintained.  Consider upgrading cmake to a newer\n",
            "  version, as this may become a fatal error in the future.\n",
            "\n",
            "\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Proceeding with version: 22.10.26.30\n",
            "-- CMAKE_CXX_FLAGS:  -std=c++0x -Wall -pedantic -Werror -Wextra -Werror=shadow -faligned-new -Werror=implicit-fallthrough=2 -Wunused-result -Werror=unused-result -Wunused-parameter -Werror=unused-parameter -fsigned-char\n",
            "-- Found PythonInterp: /usr/bin/python3 (found suitable version \"3.7.15\", minimum required is \"3.5\") \n",
            "-- `tests/monster_test.fbs`: add generation of C++ code with '--no-includes;--gen-compare'\n",
            "-- `tests/monster_test.fbs`: add generation of binary (.bfbs) schema\n",
            "-- `tests/namespace_test/namespace_test1.fbs`: add generation of C++ code with '--no-includes;--gen-compare;--gen-name-strings'\n",
            "-- `tests/namespace_test/namespace_test2.fbs`: add generation of C++ code with '--no-includes;--gen-compare;--gen-name-strings'\n",
            "-- `tests/union_vector/union_vector.fbs`: add generation of C++ code with '--no-includes;--gen-compare;--gen-name-strings'\n",
            "-- `tests/optional_scalars.fbs`: add generation of C++ code with '--no-includes;--gen-compare'\n",
            "-- `tests/native_type_test.fbs`: add generation of C++ code with ''\n",
            "-- `tests/arrays_test.fbs`: add generation of C++ code with '--scoped-enums;--gen-compare'\n",
            "-- `tests/arrays_test.fbs`: add generation of binary (.bfbs) schema\n",
            "-- `tests/monster_test.fbs`: add generation of C++ embedded binary schema code with '--no-includes;--gen-compare'\n",
            "-- `tests/monster_extra.fbs`: add generation of C++ code with '--no-includes;--gen-compare'\n",
            "-- `samples/monster.fbs`: add generation of C++ code with '--no-includes;--gen-compare'\n",
            "-- `samples/monster.fbs`: add generation of binary (.bfbs) schema\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/flatbuffers\n",
            "\u001b[35m\u001b[1mScanning dependencies of target flatc\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_parser.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/reflection.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/util.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_csharp.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_dart.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_kotlin.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_go.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_java.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_ts.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_php.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_python.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_lobster.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_lua.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_rust.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_fbs.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_json_schema.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/idl_gen_swift.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/flatc.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/flatc_main.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/bfbs_gen_lua.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/src/code_generators.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/grpc/src/compiler/cpp_generator.cc.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/grpc/src/compiler/go_generator.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/grpc/src/compiler/java_generator.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/grpc/src/compiler/python_generator.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/grpc/src/compiler/swift_generator.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/flatc.dir/grpc/src/compiler/ts_generator.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable flatc\u001b[0m\n",
            "CMakeFiles/flatc.dir/src/flatc.cpp.o: In function `flatbuffers::FlatCompiler::AnnotateBinaries(unsigned char const*, unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)':\n",
            "flatc.cpp:(.text+0x10c7): undefined reference to `flatbuffers::BinaryAnnotator::Annotate()'\n",
            "flatc.cpp:(.text+0x11be): undefined reference to `flatbuffers::AnnotatedBinaryTextGenerator::Generate(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\n",
            "CMakeFiles/flatc.dir/src/flatc_main.cpp.o: In function `main':\n",
            "flatc_main.cpp:(.text+0x241): undefined reference to `flatbuffers::NewNimBfbsGenerator(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\n",
            "collect2: error: ld returned 1 exit status\n",
            "CMakeFiles/flatc.dir/build.make:518: recipe for target 'flatc' failed\n",
            "make[2]: *** [flatc] Error 1\n",
            "CMakeFiles/Makefile2:115: recipe for target 'CMakeFiles/flatc.dir/all' failed\n",
            "make[1]: *** [CMakeFiles/flatc.dir/all] Error 2\n",
            "Makefile:140: recipe for target 'all' failed\n",
            "make: *** [all] Error 2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake==3.15.3 in /usr/local/lib/python3.7/dist-packages (3.15.3)\n"
          ]
        }
      ],
      "source": [
        " !sudo apt-get install -y build-essential autoconf libtool pkg-config \\\n",
        "     python3-dev python3-pip python3-numpy git libncurses5-dev \\\n",
        "     libzmq3-dev flex bison\n",
        "!git clone https://github.com/google/flatbuffers.git\n",
        "!cd flatbuffers && cmake -G \"Unix Makefiles\" && make && sudo make install\n",
        "!pip install cmake==3.15.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-mNLV8xvy9",
        "outputId": "b4e0df34-7bcb-42e5-cfd9-13de8d84e032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nle in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: gym>=0.15 in /usr/local/lib/python3.7/dist-packages (from nle) (0.25.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from nle) (2.10.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nle) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: minihack in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from minihack) (0.25.2)\n",
            "Requirement already satisfied: nle>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from minihack) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from minihack) (1.21.6)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from nle>=0.8.0->minihack) (2.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nle\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install minihack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyOZYbq0KFb7"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import minihack\n",
        "from minihack import RewardManager\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "import cv2 as cv \n",
        "from google.colab.patches import cv2_imshow # for image display\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1vsjUgQ2uH3",
        "outputId": "263bd521-e060-4540-8319-6c7c26ea55f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (21, 79)\u001b[0m\n",
            "  \"A Box observation space has an unconventional shape (neither an image, nor a 1D vector). \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (9, 9)\u001b[0m\n",
            "  \"A Box observation space has an unconventional shape (neither an image, nor a 1D vector). \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/DQN\n"
          ]
        }
      ],
      "source": [
        "reward_manager = RewardManager()\n",
        "reward_manager.add_kill_event(\"minotaur\", reward=5, repeatable=False)\n",
        "\n",
        "\n",
        "env = gym.make(\n",
        "    \"MiniHack-Quest-Hard-v0\", \n",
        "    observation_keys=(\"glyphs\", \"chars\", \"colors\",  \"colors_crop\", \"pixel\", \"pixel_crop\", \"blstats\"),\n",
        "    reward_manager=reward_manager, savedir=\"/content/RL/DQN/video\",\n",
        ")\n",
        "\n",
        "\n",
        "drive.mount('/content/DQN', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmqekmA5u39_"
      },
      "source": [
        "Functions required to plot a graph of the average reward vs time steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TzTR8QGyG89"
      },
      "source": [
        "# Buffer/Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ize4Jjg5Qiv"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',('state', 'state_extra_features', 'action', 'next_state', 'next_state_extra_features', 'reward'))\n",
        "\n",
        "class Buffer(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size): \n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYky_fzmyTsl"
      },
      "source": [
        "# Visited States Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_KAJWpLwbsF"
      },
      "source": [
        "Functions used to get extra information from the environment by extracting information from our visited matrix. Finds value of adjacent states to our agents current state and checks how recently they were visited as well if they were visited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLfFsdsxJ0ic"
      },
      "outputs": [],
      "source": [
        "def update_visited(visited, curr_pos):\n",
        "    visited = visited+1.0\n",
        "    visited[curr_pos[0]][curr_pos[1]] = 0.0\n",
        "    for i in range(29, 79):\n",
        "        visited[3][i] = 0.0\n",
        "        visited[19][i] = 0.0\n",
        "    return visited\n",
        "\n",
        "def adjacent_cells(visited_matrix, currentPos):\n",
        "    minInd = -1\n",
        "    minPos = [-1, -1]\n",
        "    cellInfo = np.zeros(12)\n",
        "    minVal = 100\n",
        "    count = 0\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            currentAdjacent = [currentPos[0], currentPos[1]]\n",
        "            \n",
        "            if j == 0:\n",
        "              currentAdjacent[i] = currentAdjacent[i]-1\n",
        "            else:\n",
        "              currentAdjacent[i] = currentAdjacent[i]+1\n",
        "            \n",
        "            if currentAdjacent[0] < 0 or currentAdjacent[1] < 0 or currentAdjacent[0] >= 21 or currentAdjacent[1] >= 79:\n",
        "                continue\n",
        "            currentAdjacentValue = min(100, visited_matrix[currentAdjacent[0]][currentAdjacent[1]])   \n",
        "            if currentAdjacentValue == 100:\n",
        "                cellInfo[count] = 1.0\n",
        "            elif currentAdjacentValue < minVal:\n",
        "                minVal = currentAdjacentValue\n",
        "                minInd = count\n",
        "                minPos = currentAdjacent \n",
        "            cellInfo[count+8] = currentAdjacentValue/100.0\n",
        "            count=count+1\n",
        "\n",
        "    if minInd != -1:\n",
        "        cellInfo[minInd+4] = 1.0\n",
        "\n",
        "    return cellInfo, minPos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHY9qRnj1qGd"
      },
      "outputs": [],
      "source": [
        "def stateInformation(currentObs, TotalSteps, visited_matrix):\n",
        "    extra_feature_info = np.array([TotalSteps/1000.0, currentObs['blstats'][0], currentObs['blstats'][1]])\n",
        "    adj_cell_info, recent_vis_cell = adjacent_cells(visited_matrix, (currentObs['blstats'][1], currentObs['blstats'][0]))\n",
        "    extra_feature_info = np.append(extra_feature_info, adj_cell_info)\n",
        "    return adj_cell_info, recent_vis_cell\n",
        "    return extra_feature_info, recent_vis_cell\n",
        "\n",
        "def stateInputModel(obs):\n",
        "    stateInput = obs['colors_crop'][3:6,3:6]\n",
        "    stateInput = np.array(stateInput)\n",
        "    stateInput = torch.from_numpy(stateInput).to(device).float().unsqueeze(0)\n",
        "    return stateInput\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMuKnXMDXvCx"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MtkmpVXvze9"
      },
      "source": [
        "Creating our neural network. Descriptions of the network are given in the paper. The forward function is used for two reasons.\n",
        "\n",
        "*   Determining the next action\n",
        "*   During batch optimisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xw_LiG-5ZkH"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, h, w, output):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=1, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        def conv_out(size, kernel_size = 1, stride = 1):\n",
        "            return (size-(kernel_size - 1)-1)//stride+1\n",
        "\n",
        "        convw = conv_out(conv_out(conv_out(w)))\n",
        "        convh = conv_out(conv_out(conv_out(h)))\n",
        "        linear_input_size = convw * convh * 32 + 12\n",
        "\n",
        "        self.head = nn.Linear(linear_input_size, output)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.cat((x,y),1)\n",
        "        return self.head(x)\n",
        "\n",
        "def epsilon_greedy(actions, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return int(random.uniform(0, len(actions)))\n",
        "    else:\n",
        "        return torch.argmax(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0kdiPVKZzuZ"
      },
      "source": [
        "# DQN Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3qNB-jXw_I0"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbgOtdrxw7Ed"
      },
      "outputs": [],
      "source": [
        "#List of our hyper-parameters which can be tuned to improve performance\n",
        "TARGET_UPDATE = 5000\n",
        "BUFFER_SIZE = 5000\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "LIMITED_ACTION_CHANGE = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GiBoEgyxRmk"
      },
      "source": [
        "##Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aay3UDmd5jof"
      },
      "outputs": [],
      "source": [
        "def learn_model(model, target_net, env, num_episodes, gamma, epsilon, l_r, action_list,seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    env.seed(seed)\n",
        "\n",
        "\n",
        "    #Initialise our model as well as the memory/buffer. \n",
        "    #Initialise a few other variables. \n",
        "    model.train()\n",
        "    buffer = Buffer(BUFFER_SIZE)\n",
        "    mazeExit = 0.0\n",
        "    lavaExit = 0.0\n",
        "    stairs = 0.0\n",
        "    killedMonster = 0.0\n",
        "    all_acc_rewards = []\n",
        "    tot_num_steps = 0\n",
        "    maxReward = -9999\n",
        "    #Loss function and optimiser. Using Stochastic Gradient Descent optimiser.\n",
        "    torch.autograd.set_detect_anomaly(False)\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=l_r)\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        run_frames = [] \n",
        "        acc_reward = 0.0\n",
        "        curr_observations = env.reset()\n",
        "        foundMazeExit = False\n",
        "        foundMazeExitRecent = False\n",
        "        foundLavaExit = False\n",
        "        foundStairs = False\n",
        "        visited_count = 0.0\n",
        "\n",
        "        #Initialise our Visited matrix.\n",
        "        visitedStates = np.full((21, 79), 100.0)\n",
        "        visitedStates = update_visited(visitedStates, (curr_observations['blstats'][1], curr_observations['blstats'][0]))\n",
        "        visited = np.full((21, 79), False)\n",
        "        state = stateInputModel(curr_observations).unsqueeze(0)\n",
        "        state_extra_features_np, recent_visit_cell = stateInformation(curr_observations, tot_num_steps, visited)\n",
        "        state_extra_features = torch.from_numpy(state_extra_features_np).to(device).float().unsqueeze(0)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            pred_action_vals = []\n",
        "            with torch.no_grad():\n",
        "                pred_action_vals = model(state, state_extra_features)\n",
        "            \n",
        "            #print(curr_observations[1])\n",
        "            # Get all current state action-values,actions,reward,next state and next state-action values.\n",
        "\n",
        "            curr_action = epsilon_greedy(pred_action_vals[0], 0.05)\n",
        "            curr_action_env_ind = action_list[curr_action]\n",
        "            curr_observations = env.step(curr_action_env_ind)\n",
        "            curr_reward = curr_observations[1]\n",
        "\n",
        "            done = curr_observations[2]\n",
        "\n",
        "            run_frames.append(curr_observations[0]['pixel'])\n",
        "\n",
        "            #Rewards\n",
        "            if done:\n",
        "              curr_reward = 0.0 \n",
        "            if curr_reward == -0.01:\n",
        "              curr_reward = -0.66\n",
        "            elif visited[curr_observations[0]['blstats'][1]][curr_observations[0]['blstats'][0]] >= 100.0:\n",
        "              curr_reward = 1.0\n",
        "              visited_count += 1.0\n",
        "            elif curr_observations[0]['blstats'][1] == recent_visit_cell[0] and curr_observations[0]['blstats'][0] == recent_visit_cell[1]:\n",
        "              curr_reward = -0.33\n",
        "\n",
        "            if curr_observations[0]['blstats'][0] == 27 and curr_observations[0]['blstats'][1] == 11 and not foundMazeExit:\n",
        "              foundMazeExit = True\n",
        "              foundMazeExitRecent = True\n",
        "              mazeExit += 1.0\n",
        "              print('Maze Exit Found')\n",
        "\n",
        "            if curr_reward == 5:\n",
        "              killedMonster += 1.0\n",
        "              print(\"Killed Monster!\")\n",
        "\n",
        "            if curr_observations[0]['blstats'][0] == 52 and curr_observations[0]['blstats'][1] == 11 and not foundMazeExit:\n",
        "              curr_reward += 5.0\n",
        "              foundLavaExit = True\n",
        "              print(\"Lava Crossed\")\n",
        "              lavaExit += 1.0\n",
        "\n",
        "            elif curr_observations[0]['blstats'][0] == 72 and curr_observations[0]['blstats'][1] == 11 and not foundMazeExit:\n",
        "              curr_reward += 5.0\n",
        "              foundStairs = True\n",
        "              print(\"Found Stairs\")\n",
        "              stairs += 1.0\n",
        "\n",
        "            if not foundMazeExit:\n",
        "                next_state_visited_value = min(100.0, visited[curr_observations[0]['blstats'][1]][curr_observations[0]['blstats'][0]])\n",
        "                curr_reward += float(0.3*(next_state_visited_value/100.0))\n",
        "\n",
        "            visited = update_visited(visited, (curr_observations[0]['blstats'][1], curr_observations[0]['blstats'][0]))\n",
        "            curr_action = torch.tensor([[curr_action]], device = device)\n",
        "            curr_reward = torch.tensor([curr_reward], device = device)\n",
        "            \n",
        "            next_state = None\n",
        "            next_state_extra_features = None\n",
        "            if not done: \n",
        "                  next_state = stateInputModel(curr_observations[0]).unsqueeze(0)\n",
        "                  next_state_extra_features_np, recent_visit_cell = stateInformation(curr_observations[0], tot_num_steps, visited)\n",
        "                  next_state_extra_features = torch.from_numpy(next_state_extra_features_np).to(device).float().unsqueeze(0)\n",
        "\n",
        "\n",
        "            buffer.push(state, state_extra_features, curr_action, next_state, next_state_extra_features, curr_reward)\n",
        "            BATCH_SIZE = 1\n",
        "            optimise_model(model, target_net, buffer, 0.0, l_r, optimizer, loss_fn)\n",
        "            \n",
        "            state = next_state\n",
        "            state_extra_features = next_state_extra_features\n",
        "            just_found_cell_after_maze = False\n",
        "            \n",
        "            tot_num_steps += 1\n",
        "            if tot_num_steps % TARGET_UPDATE == 0:\n",
        "              target_net.load_state_dict(model.state_dict())\n",
        "\n",
        "            acc_reward += float(curr_reward[0])\n",
        "            \n",
        "        all_acc_rewards.append(acc_reward)\n",
        "        if i%1 == 0:\n",
        "            print('Average Reward: ', acc_reward, \"Episodes: \",tot_num_steps) \n",
        "        \n",
        "        if(foundMazeExit):\n",
        "          maxReward = acc_reward\n",
        "          video_name = \"animation\"+str(seed)+\".mp4\"\n",
        "          fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "          video = cv.VideoWriter(video_name, fourcc, 30, (run_frames[0].shape[1], run_frames[0].shape[0]))\n",
        "          for frame in run_frames:\n",
        "            video.write(frame[:,:,::-1])\n",
        "          video.release()\n",
        "        \n",
        "    return all_acc_rewards, np.array([mazeExit, lavaExit, stairs, killedMonster])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J_qLAR8xYj1"
      },
      "source": [
        "##Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVhK-e1X-ofw"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1\n",
        "def optimise_model(model, target_net, buffer, gamma,l_r ,optimizer, loss_fn):\n",
        "    if len(buffer) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = buffer.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    non_final_next_statesL = [s for s in batch.next_state if s is not None]\n",
        "    if non_final_next_statesL != []:\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        non_final_next_states_extra = torch.cat([s for s in batch.next_state_extra_features if s is not None])\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states, non_final_next_states_extra).max(1)[0].detach()\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    state_extra_features_batch = torch.cat(batch.state_extra_features)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    state_action_values = model(state_batch, state_extra_features_batch).gather(1, action_batch)\n",
        "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "    loss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep4erytmD4kE"
      },
      "source": [
        "# Runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24E9hja7QVTo"
      },
      "outputs": [],
      "source": [
        "def moving_average(a, n):\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret / n\n",
        "\n",
        "\n",
        "def get_avg_reward_for_multiple_runs():\n",
        "    action_ind_list =  [0,1,2,3,4,5,6,7,20,33,47,49,50,51,52,58,72,75]\n",
        "    n_runs = 3\n",
        "    np.random.seed(53)\n",
        "    seeds = np.random.randint(1000, size=n_runs)\n",
        "    num_episodes = 100\n",
        "    avged_rewards = np.zeros(num_episodes)\n",
        "    all_counts = np.zeros(4)\n",
        "    total_scores = np.zeros(num_episodes)\n",
        "    for run in range(n_runs):\n",
        "      env.reset()\n",
        "      model = CNN(3, 3, 4).to(device)\n",
        "      target_net = CNN(3, 3, 4).to(device)\n",
        "      target_net.load_state_dict(model.state_dict())\n",
        "      target_net.eval()\n",
        "      acc_rew, counts = learn_model(model, target_net, env, num_episodes, 0.999, 0.1, 0.001, action_ind_list,seed = int(seeds[run]))\n",
        "      all_counts = all_counts + counts\n",
        "      total_scores += acc_rew\n",
        "\n",
        "    episodes = list(range(num_episodes))\n",
        "    mean = total_scores / n_runs\n",
        "    std = np.std(mean)\n",
        "    window_size = 10\n",
        "    moving_avg = moving_average(mean, window_size)\n",
        "\n",
        "    plt.plot(episodes, moving_avg, color='b')\n",
        "    plt.fill_between(episodes, moving_avg + std, moving_avg - std, facecolor='blue', alpha=0.2)\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.title('DQN averaged over 3 seeds')\n",
        "    plt.show()\n",
        "\n",
        "    return all_counts,total_scores,n_runs,episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP-8fKghRejI",
        "outputId": "1290467c-1e2e-434b-94d9-8f8bea5a1d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Reward:  -409.90600981516764 Episodes:  1000\n",
            "Average Reward:  -266.43199920095503 Episodes:  2000\n",
            "Average Reward:  -338.77200686745346 Episodes:  3000\n",
            "Average Reward:  -141.84799988334998 Episodes:  4000\n",
            "Average Reward:  -51.59000208135694 Episodes:  5000\n",
            "Average Reward:  -129.3840081174858 Episodes:  6000\n",
            "Average Reward:  24.400993367191404 Episodes:  7000\n",
            "Average Reward:  -194.29200997995213 Episodes:  8000\n",
            "Average Reward:  74.32699213223532 Episodes:  9000\n",
            "Average Reward:  -33.575007104314864 Episodes:  10000\n",
            "Average Reward:  203.1459868149832 Episodes:  11000\n",
            "Average Reward:  -135.66800958802924 Episodes:  12000\n",
            "Average Reward:  52.76998952077702 Episodes:  13000\n",
            "Average Reward:  394.2359838630073 Episodes:  14000\n",
            "Average Reward:  199.89798469329253 Episodes:  15000\n",
            "Maze Exit Found\n",
            "Average Reward:  17.14598749484867 Episodes:  16000\n",
            "Average Reward:  312.965981926769 Episodes:  17000\n",
            "Average Reward:  431.66898017702624 Episodes:  18000\n",
            "Average Reward:  -93.70200763316825 Episodes:  19000\n",
            "Maze Exit Found\n",
            "Average Reward:  457.5859812023118 Episodes:  19954\n",
            "Maze Exit Found\n",
            "Average Reward:  29.085982813034207 Episodes:  20954\n",
            "Average Reward:  97.23698742641136 Episodes:  21954\n",
            "Average Reward:  -92.95101151848212 Episodes:  22954\n",
            "Average Reward:  372.5709757343866 Episodes:  23954\n",
            "Maze Exit Found\n",
            "Average Reward:  404.0519865308888 Episodes:  24954\n",
            "Maze Exit Found\n",
            "Average Reward:  317.10198337072507 Episodes:  25832\n",
            "Maze Exit Found\n",
            "Average Reward:  398.1529786833562 Episodes:  26801\n",
            "Average Reward:  505.35297926841304 Episodes:  27801\n",
            "Maze Exit Found\n",
            "Average Reward:  -31.31702140113339 Episodes:  28801\n",
            "Average Reward:  582.9169786386192 Episodes:  29801\n",
            "Maze Exit Found\n",
            "Average Reward:  397.91597675625235 Episodes:  30801\n",
            "Maze Exit Found\n",
            "Average Reward:  186.44498836435378 Episodes:  31630\n",
            "Maze Exit Found\n",
            "Average Reward:  370.2849890515208 Episodes:  32630\n",
            "Maze Exit Found\n",
            "Average Reward:  543.1269825706258 Episodes:  33630\n",
            "Average Reward:  685.7279746169224 Episodes:  34630\n",
            "Maze Exit Found\n",
            "Average Reward:  -314.51102120103315 Episodes:  35595\n",
            "Maze Exit Found\n",
            "Average Reward:  354.3919950174168 Episodes:  36595\n",
            "Maze Exit Found\n",
            "Average Reward:  563.4529767394997 Episodes:  37595\n",
            "Maze Exit Found\n",
            "Average Reward:  66.39599457988515 Episodes:  37887\n",
            "Maze Exit Found\n",
            "Average Reward:  310.43599564861506 Episodes:  38887\n",
            "Maze Exit Found\n",
            "Average Reward:  180.52199449576437 Episodes:  39291\n",
            "Average Reward:  -102.6020063557662 Episodes:  40291\n",
            "Maze Exit Found\n",
            "Average Reward:  501.91397684114054 Episodes:  41291\n",
            "Average Reward:  559.797979010269 Episodes:  42291\n",
            "Maze Exit Found\n",
            "Average Reward:  132.0689962678589 Episodes:  42522\n",
            "Average Reward:  449.2439839011058 Episodes:  43522\n",
            "Average Reward:  518.9779812470078 Episodes:  44522\n",
            "Average Reward:  545.2979807020165 Episodes:  45522\n",
            "Maze Exit Found\n",
            "Average Reward:  527.2139899297617 Episodes:  46522\n",
            "Maze Exit Found\n",
            "Average Reward:  557.6879896749742 Episodes:  47522\n",
            "Maze Exit Found\n",
            "Average Reward:  -299.40801964048296 Episodes:  48522\n",
            "Average Reward:  515.4899821211584 Episodes:  49522\n",
            "Maze Exit Found\n",
            "Average Reward:  291.11199399549514 Episodes:  50522\n",
            "Maze Exit Found\n",
            "Average Reward:  -409.3850216702558 Episodes:  51522\n",
            "Average Reward:  563.5699772918597 Episodes:  52522\n",
            "Average Reward:  594.4189785425551 Episodes:  53522\n",
            "Average Reward:  -129.19000645680353 Episodes:  54522\n",
            "Maze Exit Found\n",
            "Average Reward:  350.96098458208144 Episodes:  55522\n",
            "Maze Exit Found\n",
            "Average Reward:  492.1179921515286 Episodes:  56522\n",
            "Average Reward:  381.5189862237312 Episodes:  57522\n",
            "Maze Exit Found\n",
            "Average Reward:  467.74098726315424 Episodes:  58522\n",
            "Maze Exit Found\n",
            "Average Reward:  20.649977243039757 Episodes:  59522\n",
            "Maze Exit Found\n",
            "Average Reward:  41.285989731084555 Episodes:  60074\n",
            "Maze Exit Found\n",
            "Average Reward:  345.8769818278961 Episodes:  61074\n",
            "Average Reward:  580.9639789657667 Episodes:  62074\n",
            "Average Reward:  506.7099804673344 Episodes:  63074\n",
            "Maze Exit Found\n",
            "Average Reward:  107.35799719952047 Episodes:  63326\n",
            "Maze Exit Found\n",
            "Average Reward:  -94.05001126788557 Episodes:  63903\n",
            "Maze Exit Found\n",
            "Average Reward:  66.55499867722392 Episodes:  64117\n",
            "Maze Exit Found\n",
            "Average Reward:  180.82297818595544 Episodes:  65117\n",
            "Maze Exit Found\n",
            "Average Reward:  108.9289800110273 Episodes:  65997\n",
            "Maze Exit Found\n",
            "Average Reward:  384.5379909002222 Episodes:  66997\n",
            "Average Reward:  600.7429774855264 Episodes:  67997\n",
            "Maze Exit Found\n",
            "Average Reward:  220.84499020176008 Episodes:  68480\n",
            "Maze Exit Found\n",
            "Average Reward:  -263.3210208513774 Episodes:  69433\n",
            "Average Reward:  378.310980160255 Episodes:  70433\n",
            "Maze Exit Found\n",
            "Average Reward:  441.51998310908675 Episodes:  71433\n",
            "Average Reward:  456.3979809740558 Episodes:  72433\n",
            "Average Reward:  475.19498318620026 Episodes:  73433\n",
            "Maze Exit Found\n",
            "Average Reward:  388.89598878705874 Episodes:  74433\n",
            "Maze Exit Found\n",
            "Average Reward:  553.0559769831598 Episodes:  75433\n",
            "Maze Exit Found\n",
            "Average Reward:  125.52298335172236 Episodes:  76433\n",
            "Maze Exit Found\n",
            "Average Reward:  246.4409881690517 Episodes:  77000\n",
            "Maze Exit Found\n",
            "Average Reward:  418.90798546839505 Episodes:  77847\n",
            "Maze Exit Found\n",
            "Average Reward:  644.343973598443 Episodes:  78847\n",
            "Average Reward:  583.9829774880782 Episodes:  79847\n",
            "Average Reward:  493.1559778894298 Episodes:  80847\n",
            "Maze Exit Found\n",
            "Average Reward:  -35.34100514790043 Episodes:  81202\n",
            "Average Reward:  619.4019760894589 Episodes:  82202\n",
            "Maze Exit Found\n",
            "Average Reward:  195.76499329833314 Episodes:  83202\n",
            "Average Reward:  555.6499786768109 Episodes:  84202\n",
            "Average Reward:  376.66698615672067 Episodes:  85202\n",
            "Average Reward:  552.9109769649804 Episodes:  86202\n",
            "Maze Exit Found\n",
            "Average Reward:  146.69199477462098 Episodes:  86528\n",
            "Maze Exit Found\n",
            "Average Reward:  225.9229761455208 Episodes:  87528\n",
            "Average Reward:  393.7129854550585 Episodes:  88528\n",
            "Average Reward:  667.7339740563184 Episodes:  89528\n",
            "Maze Exit Found\n",
            "Average Reward:  495.8109895158559 Episodes:  90528\n",
            "Maze Exit Found\n",
            "Average Reward:  -53.9510086895898 Episodes:  90965\n",
            "Maze Exit Found\n",
            "Average Reward:  187.27297919103876 Episodes:  91965\n",
            "Average Reward:  -572.5770182088017 Episodes:  1000\n",
            "Average Reward:  -367.4319998137653 Episodes:  2000\n",
            "Average Reward:  -349.0839996859431 Episodes:  3000\n",
            "Average Reward:  -284.9209951357916 Episodes:  4000\n",
            "Average Reward:  -522.1730162482709 Episodes:  5000\n",
            "Average Reward:  -260.46399648021907 Episodes:  6000\n",
            "Average Reward:  -508.120015387889 Episodes:  7000\n",
            "Average Reward:  -511.50001963507384 Episodes:  8000\n",
            "Average Reward:  -217.31700762407854 Episodes:  9000\n",
            "Average Reward:  -91.01699990360066 Episodes:  10000\n",
            "Average Reward:  48.51099506067112 Episodes:  11000\n",
            "Average Reward:  -112.6120067187585 Episodes:  12000\n",
            "Average Reward:  -108.8700032797642 Episodes:  13000\n",
            "Average Reward:  -163.62600845796987 Episodes:  14000\n",
            "Average Reward:  181.82299094786867 Episodes:  15000\n",
            "Average Reward:  -63.613006497733295 Episodes:  16000\n",
            "Average Reward:  -30.66100826812908 Episodes:  17000\n",
            "Average Reward:  56.705990098416805 Episodes:  18000\n",
            "Average Reward:  -164.47400678647682 Episodes:  19000\n",
            "Average Reward:  200.53998117754236 Episodes:  20000\n",
            "Average Reward:  -74.79100979445502 Episodes:  21000\n",
            "Average Reward:  285.8349880860187 Episodes:  22000\n",
            "Average Reward:  -54.585012251045555 Episodes:  23000\n",
            "Average Reward:  353.7799839912914 Episodes:  24000\n",
            "Average Reward:  -6.911010338459164 Episodes:  25000\n",
            "Average Reward:  185.84098578058183 Episodes:  26000\n",
            "Average Reward:  -161.93600627500564 Episodes:  27000\n",
            "Average Reward:  428.41098268702626 Episodes:  28000\n",
            "Average Reward:  283.51898470986634 Episodes:  29000\n",
            "Maze Exit Found\n",
            "Average Reward:  215.68698193877935 Episodes:  30000\n",
            "Maze Exit Found\n",
            "Average Reward:  114.8829843676649 Episodes:  31000\n",
            "Maze Exit Found\n",
            "Average Reward:  346.84397464664653 Episodes:  32000\n",
            "Maze Exit Found\n",
            "Average Reward:  327.73299057642 Episodes:  33000\n",
            "Average Reward:  472.3919795304537 Episodes:  34000\n",
            "Maze Exit Found\n",
            "Average Reward:  -24.446022080723196 Episodes:  35000\n",
            "Average Reward:  490.5479767797515 Episodes:  36000\n",
            "Maze Exit Found\n",
            "Average Reward:  388.21898369677365 Episodes:  36880\n",
            "Maze Exit Found\n",
            "Average Reward:  30.327997672371566 Episodes:  37126\n",
            "Average Reward:  49.739988377317786 Episodes:  38126\n",
            "Maze Exit Found\n",
            "Average Reward:  102.77999241510406 Episodes:  38487\n",
            "Maze Exit Found\n",
            "Average Reward:  17.71999857155606 Episodes:  38669\n",
            "Maze Exit Found\n",
            "Average Reward:  163.00598321575671 Episodes:  39669\n",
            "Average Reward:  396.01297943200916 Episodes:  40669\n",
            "Maze Exit Found\n",
            "Average Reward:  492.43297712365165 Episodes:  41669\n",
            "Average Reward:  470.6869824146852 Episodes:  42669\n",
            "Average Reward:  449.3819837225601 Episodes:  43669\n",
            "Average Reward:  514.6559780891985 Episodes:  44669\n",
            "Average Reward:  54.349977016914636 Episodes:  45669\n",
            "Average Reward:  538.2159794108011 Episodes:  46669\n",
            "Maze Exit Found\n",
            "Average Reward:  170.61997339129448 Episodes:  47669\n",
            "Maze Exit Found\n",
            "Average Reward:  527.6899847183377 Episodes:  48669\n",
            "Average Reward:  544.1099801850505 Episodes:  49669\n",
            "Average Reward:  566.1809789785184 Episodes:  50669\n",
            "Maze Exit Found\n",
            "Average Reward:  517.2549840817228 Episodes:  51669\n",
            "Maze Exit Found\n",
            "Average Reward:  489.5779913510196 Episodes:  52669\n",
            "Average Reward:  740.0919729261659 Episodes:  53669\n",
            "Maze Exit Found\n",
            "Average Reward:  416.6619987706654 Episodes:  54669\n",
            "Maze Exit Found\n",
            "Average Reward:  343.80399125348777 Episodes:  55669\n",
            "Maze Exit Found\n",
            "Average Reward:  467.0679878080264 Episodes:  56669\n",
            "Average Reward:  534.5859804106876 Episodes:  57669\n",
            "Average Reward:  533.5949807865545 Episodes:  58669\n",
            "Average Reward:  525.3209814648144 Episodes:  59669\n",
            "Maze Exit Found\n",
            "Average Reward:  471.3939956203103 Episodes:  60669\n",
            "Maze Exit Found\n",
            "Average Reward:  527.3609882807359 Episodes:  61669\n",
            "Average Reward:  592.4359787958674 Episodes:  62669\n",
            "Maze Exit Found\n",
            "Average Reward:  577.0939800092019 Episodes:  63669\n",
            "Average Reward:  569.7849790598266 Episodes:  64669\n",
            "Average Reward:  647.8919764207676 Episodes:  65669\n",
            "Average Reward:  524.592981449794 Episodes:  66669\n",
            "Maze Exit Found\n",
            "Average Reward:  547.4939816067927 Episodes:  67669\n",
            "Maze Exit Found\n",
            "Average Reward:  394.7179985092953 Episodes:  68669\n",
            "Maze Exit Found\n",
            "Average Reward:  380.3689895076677 Episodes:  69669\n",
            "Average Reward:  580.9129785755649 Episodes:  70669\n",
            "Maze Exit Found\n",
            "Average Reward:  364.1619938449003 Episodes:  71669\n",
            "Maze Exit Found\n",
            "Average Reward:  583.3549900935031 Episodes:  72669\n",
            "Maze Exit Found\n",
            "Average Reward:  549.4089892837219 Episodes:  73669\n",
            "Maze Exit Found\n",
            "Average Reward:  458.23198771849275 Episodes:  74669\n",
            "Maze Exit Found\n",
            "Average Reward:  509.53299436252564 Episodes:  75669\n",
            "Maze Exit Found\n",
            "Average Reward:  388.25199532834813 Episodes:  76669\n",
            "Average Reward:  633.4909769259393 Episodes:  77669\n",
            "Average Reward:  513.0859815864824 Episodes:  78669\n",
            "Average Reward:  578.5139788226224 Episodes:  79669\n",
            "Average Reward:  599.4259779737331 Episodes:  80669\n",
            "Average Reward:  608.125977977179 Episodes:  81669\n",
            "Maze Exit Found\n",
            "Average Reward:  369.65699592232704 Episodes:  82669\n",
            "Average Reward:  701.481974852737 Episodes:  83669\n",
            "Average Reward:  732.1619730065577 Episodes:  84669\n",
            "Maze Exit Found\n",
            "Average Reward:  -30.272023546975106 Episodes:  85669\n",
            "Maze Exit Found\n",
            "Average Reward:  586.2299805637449 Episodes:  86669\n",
            "Maze Exit Found\n",
            "Average Reward:  416.68299873312935 Episodes:  87669\n",
            "Maze Exit Found\n",
            "Average Reward:  525.588990429882 Episodes:  88669\n",
            "Maze Exit Found\n",
            "Average Reward:  514.640984909609 Episodes:  89669\n",
            "Average Reward:  639.2629759991542 Episodes:  90669\n",
            "Maze Exit Found\n",
            "Average Reward:  570.0729865408503 Episodes:  91669\n",
            "Average Reward:  754.8019716404378 Episodes:  92669\n",
            "Maze Exit Found\n",
            "Average Reward:  451.631991866976 Episodes:  93669\n",
            "Maze Exit Found\n",
            "Average Reward:  634.8449812014587 Episodes:  94669\n",
            "Maze Exit Found\n",
            "Average Reward:  629.1709817163646 Episodes:  95669\n",
            "Maze Exit Found\n",
            "Average Reward:  630.2869750969112 Episodes:  96669\n",
            "Maze Exit Found\n",
            "Average Reward:  31.186993377283216 Episodes:  97019\n",
            "Average Reward:  -405.59900416992605 Episodes:  1000\n",
            "Average Reward:  -281.5669961501844 Episodes:  2000\n",
            "Average Reward:  -325.82299354160205 Episodes:  3000\n",
            "Average Reward:  -271.8779951413162 Episodes:  4000\n",
            "Average Reward:  -228.00299625704065 Episodes:  5000\n",
            "Average Reward:  -224.55999909574166 Episodes:  6000\n",
            "Average Reward:  -252.59600445395336 Episodes:  7000\n",
            "Average Reward:  -199.8250008863397 Episodes:  8000\n",
            "Average Reward:  31.991995943244547 Episodes:  9000\n",
            "Average Reward:  -121.07200449472293 Episodes:  10000\n",
            "Average Reward:  -108.94800802692771 Episodes:  11000\n",
            "Average Reward:  -162.84701130073518 Episodes:  12000\n",
            "Average Reward:  -113.30300000915304 Episodes:  13000\n",
            "Average Reward:  336.6489864503965 Episodes:  14000\n",
            "Average Reward:  -77.29000788321719 Episodes:  15000\n",
            "Average Reward:  409.3829813511111 Episodes:  16000\n",
            "Average Reward:  273.668984582182 Episodes:  17000\n",
            "Average Reward:  418.6989792329259 Episodes:  18000\n",
            "Average Reward:  302.3089801254682 Episodes:  19000\n",
            "Average Reward:  287.8979803742841 Episodes:  20000\n",
            "Average Reward:  413.3879850269295 Episodes:  21000\n",
            "Maze Exit Found\n",
            "Average Reward:  369.95599262369797 Episodes:  22000\n",
            "Maze Exit Found\n",
            "Average Reward:  -191.72701899288222 Episodes:  23000\n",
            "Average Reward:  479.90098053216934 Episodes:  24000\n",
            "Average Reward:  333.6459853630513 Episodes:  25000\n",
            "Maze Exit Found\n",
            "Average Reward:  71.60497920727357 Episodes:  26000\n",
            "Maze Exit Found\n",
            "Average Reward:  2.6139881345443428 Episodes:  27000\n",
            "Average Reward:  492.07998057221994 Episodes:  28000\n",
            "Average Reward:  593.07197839953 Episodes:  29000\n",
            "Maze Exit Found\n",
            "Average Reward:  298.58699130499735 Episodes:  30000\n",
            "Maze Exit Found\n",
            "Average Reward:  309.6019854899496 Episodes:  30705\n",
            "Average Reward:  340.33098473679274 Episodes:  31705\n",
            "Maze Exit Found\n",
            "Average Reward:  170.89899424882606 Episodes:  32015\n",
            "Average Reward:  -141.1440061996691 Episodes:  33015\n",
            "Average Reward:  319.80497632641345 Episodes:  34015\n",
            "Average Reward:  456.385978102684 Episodes:  35015\n",
            "Maze Exit Found\n",
            "Average Reward:  515.5679776910692 Episodes:  36015\n",
            "Maze Exit Found\n",
            "Average Reward:  546.719977797009 Episodes:  37015\n",
            "Average Reward:  496.35598160885274 Episodes:  38015\n",
            "Average Reward:  469.3169829468243 Episodes:  39015\n",
            "Average Reward:  524.5559814269654 Episodes:  40015\n",
            "Maze Exit Found\n",
            "Average Reward:  -379.10702103562653 Episodes:  41015\n",
            "Maze Exit Found\n",
            "Average Reward:  86.72097835037857 Episodes:  42015\n",
            "Average Reward:  664.4639756903052 Episodes:  43015\n",
            "Average Reward:  596.0199771076441 Episodes:  44015\n",
            "Maze Exit Found\n",
            "Average Reward:  73.17499737534672 Episodes:  44208\n",
            "Maze Exit Found\n",
            "Average Reward:  266.6629868731834 Episodes:  44888\n",
            "Maze Exit Found\n",
            "Average Reward:  379.5639841533266 Episodes:  45613\n",
            "Maze Exit Found\n",
            "Average Reward:  110.56299930857494 Episodes:  46002\n",
            "Average Reward:  -119.868006976787 Episodes:  47002\n",
            "Average Reward:  188.6059712707065 Episodes:  48002\n",
            "Average Reward:  280.6679831170477 Episodes:  49002\n",
            "Average Reward:  577.6419762838632 Episodes:  50002\n",
            "Average Reward:  502.727981609758 Episodes:  51002\n",
            "Average Reward:  657.1709759887308 Episodes:  52002\n",
            "Average Reward:  480.68698259582743 Episodes:  53002\n",
            "Average Reward:  574.5829793377779 Episodes:  54002\n",
            "Maze Exit Found\n",
            "Average Reward:  236.54598098713905 Episodes:  54912\n",
            "Average Reward:  454.4019834906794 Episodes:  55912\n",
            "Maze Exit Found\n",
            "Average Reward:  249.22998431744054 Episodes:  56731\n",
            "Average Reward:  569.8989784247242 Episodes:  57731\n",
            "Average Reward:  652.6229752986692 Episodes:  58731\n",
            "Average Reward:  550.99098024657 Episodes:  59731\n",
            "Maze Exit Found\n",
            "Average Reward:  490.3299904852174 Episodes:  60731\n",
            "Average Reward:  423.6619850494899 Episodes:  61731\n",
            "Average Reward:  -136.3810062999837 Episodes:  62731\n",
            "Maze Exit Found\n",
            "Average Reward:  329.06198895862326 Episodes:  63233\n",
            "Maze Exit Found\n",
            "Average Reward:  499.91898246854544 Episodes:  64164\n",
            "Maze Exit Found\n",
            "Average Reward:  237.37597671383992 Episodes:  65164\n",
            "Average Reward:  433.68697970639914 Episodes:  66164\n",
            "Average Reward:  410.27997877588496 Episodes:  67164\n",
            "Maze Exit Found\n",
            "Average Reward:  458.1139989392832 Episodes:  68164\n",
            "Average Reward:  548.868979995139 Episodes:  69164\n",
            "Maze Exit Found\n",
            "Average Reward:  378.28699856298044 Episodes:  70164\n",
            "Maze Exit Found\n",
            "Average Reward:  608.4559819372371 Episodes:  71164\n",
            "Maze Exit Found\n",
            "Average Reward:  580.1159814316779 Episodes:  72164\n",
            "Average Reward:  381.57798596378416 Episodes:  73164\n",
            "Average Reward:  577.7749790935777 Episodes:  74164\n",
            "Average Reward:  480.1469824933447 Episodes:  75164\n",
            "Maze Exit Found\n",
            "Average Reward:  -32.456022469792515 Episodes:  76164\n",
            "Maze Exit Found\n",
            "Average Reward:  347.2789956559427 Episodes:  77164\n",
            "Maze Exit Found\n",
            "Average Reward:  51.0009970087558 Episodes:  77473\n",
            "Maze Exit Found\n",
            "Average Reward:  425.7119815144688 Episodes:  78473\n",
            "Average Reward:  531.0739785251208 Episodes:  79473\n",
            "Maze Exit Found\n",
            "Average Reward:  251.636990820989 Episodes:  79905\n",
            "Average Reward:  531.033980504144 Episodes:  80905\n",
            "Maze Exit Found\n",
            "Average Reward:  213.13099210290238 Episodes:  81300\n",
            "Average Reward:  457.1349765053019 Episodes:  82300\n",
            "Maze Exit Found\n",
            "Average Reward:  87.42999791773036 Episodes:  82479\n",
            "Average Reward:  475.550979945343 Episodes:  83479\n",
            "Average Reward:  331.64398497901857 Episodes:  84479\n",
            "Average Reward:  626.6349773332477 Episodes:  85479\n",
            "Maze Exit Found\n",
            "Average Reward:  183.29399344697595 Episodes:  85801\n",
            "Maze Exit Found\n",
            "Average Reward:  247.37797989416867 Episodes:  86745\n",
            "Maze Exit Found\n",
            "Average Reward:  409.27698567602783 Episodes:  87341\n",
            "Maze Exit Found\n",
            "Average Reward:  187.12498844927177 Episodes:  87895\n",
            "Average Reward:  583.9669779222459 Episodes:  88895\n",
            "Average Reward:  495.04998255195096 Episodes:  89895\n",
            "Maze Exit Found\n",
            "Average Reward:  73.13599305227399 Episodes:  90270\n",
            "Average Reward:  527.0619781725109 Episodes:  91270\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "all_counts,total_scores,n_runs,episodes = get_avg_reward_for_multiple_runs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8YvYeE1SDXY"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (10, 5))\n",
        "subtasks = [\"Maze Exit\",\"Crossed lava river\",\"Defeated Demon\",\"Staircase\"]\n",
        "# creating the bar plot\n",
        "plt.bar(subtasks, all_counts1, color ='maroon',\n",
        "        width = 0.4)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel(\"Sub tasks\")\n",
        "plt.ylabel(\"Times Completed\")\n",
        "plt.title(\"Number of Times Subtasks have been completed per 100 episodes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ2JsLq4sks3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "df = pd.DataFrame(total_scores,columns=[\"Rewards\"])\n",
        "df.to_csv('output.csv') \n",
        "files.download('output.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}